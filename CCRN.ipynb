{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "executionInfo": {
     "elapsed": 1503,
     "status": "ok",
     "timestamp": 1733641278831,
     "user": {
      "displayName": "Sanele Hlabisa",
      "userId": "02109846400367384167"
     },
     "user_tz": -120
    },
    "id": "lGX3qKuVToKx",
    "papermill": {
     "duration": 0.00601,
     "end_time": "2025-09-03T13:26:19.329503",
     "exception": false,
     "start_time": "2025-09-03T13:26:19.323493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "YezEobrrTq25",
    "papermill": {
     "duration": 0.00614,
     "end_time": "2025-09-03T13:26:19.341158",
     "exception": false,
     "start_time": "2025-09-03T13:26:19.335018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CRNN implemented using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "executionInfo": {
     "elapsed": 29289,
     "status": "ok",
     "timestamp": 1733641308116,
     "user": {
      "displayName": "Sanele Hlabisa",
      "userId": "02109846400367384167"
     },
     "user_tz": -120
    },
    "id": "z5AUavMqTzfb",
    "papermill": {
     "duration": 5.706252,
     "end_time": "2025-09-03T13:26:25.052511",
     "exception": false,
     "start_time": "2025-09-03T13:26:19.346259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn import CTCLoss\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "executionInfo": {
     "elapsed": 24172,
     "status": "ok",
     "timestamp": 1733641332284,
     "user": {
      "displayName": "Sanele Hlabisa",
      "userId": "02109846400367384167"
     },
     "user_tz": -120
    },
    "id": "gcGNScI0BplH",
    "outputId": "1409a3e3-eb9e-4a2a-a927-263727eea134",
    "papermill": {
     "duration": 0.0052,
     "end_time": "2025-09-03T13:26:25.063426",
     "exception": false,
     "start_time": "2025-09-03T13:26:25.058226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1733641332284,
     "user": {
      "displayName": "Sanele Hlabisa",
      "userId": "02109846400367384167"
     },
     "user_tz": -120
    },
    "id": "bD_g2KqnaJ6r",
    "papermill": {
     "duration": 0.012333,
     "end_time": "2025-09-03T13:26:25.080970",
     "exception": false,
     "start_time": "2025-09-03T13:26:25.068637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "char_list = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 '\n",
    "\n",
    "max_label_len = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1733641332284,
     "user": {
      "displayName": "Sanele Hlabisa",
      "userId": "02109846400367384167"
     },
     "user_tz": -120
    },
    "id": "TueXeM-ZUC5s",
    "papermill": {
     "duration": 0.027857,
     "end_time": "2025-09-03T13:26:25.113989",
     "exception": false,
     "start_time": "2025-09-03T13:26:25.086132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Encode the label to numerical format with padding or truncation to fixed length\n",
    "def encode_to_labels(txt, char_list, target_len=11):\n",
    "    dig_lst = []\n",
    "    for char in txt:\n",
    "        try:\n",
    "            dig_lst.append(char_list.index(char))\n",
    "        except ValueError:\n",
    "            print(f\"Character not found: {char}\")\n",
    "\n",
    "    # Truncate or pad to fixed length\n",
    "    if len(dig_lst) > target_len:\n",
    "        dig_lst = dig_lst[:target_len]  # Truncate\n",
    "    else:\n",
    "        padding_token = len(char_list) - 1  # Padding token (e.g., blank character)\n",
    "        dig_lst.extend([padding_token] * (target_len - len(dig_lst)))  # Pad\n",
    "\n",
    "    return dig_lst\n",
    "\n",
    "# Custom Dataset class with augmentation and tracking\n",
    "class RecogDataset(Dataset):\n",
    "    def __init__(self, data_path, char_list, target_size=(32, 512), pred_len=11, augment=True, skip_text=True):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.label_length = []\n",
    "        self.input_length = []\n",
    "        self.orig_txt = []\n",
    "        self.char_list = char_list\n",
    "        self.pred_len = pred_len  # Fixed length of labels\n",
    "        self.augment = augment\n",
    "        self.character_counts = Counter()\n",
    "\n",
    "        # Define the transform with color augmentation\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomRotation(degrees=2),\n",
    "            transforms.RandomAffine(\n",
    "                degrees=0,\n",
    "                translate=(0.02, 0.02),\n",
    "                scale=(0.9, 1.1),\n",
    "                shear=(-2, 2),\n",
    "            ),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n",
    "            transforms.Resize(target_size),  # ðŸ”§ Ensure all images have same size\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,)),\n",
    "        ])\n",
    "\n",
    "        # Load all image paths and labels\n",
    "        for root, dirs, files in os.walk(data_path):\n",
    "            for file in files:\n",
    "                if file.endswith(('.png', '.jpg', '.jpeg')):  # Filter image files\n",
    "                    image_path = os.path.join(root, file)\n",
    "                    label = file.split('_')[1].split('.')[0]  # Extract label from filename\n",
    "                    if skip_text and (len(label) != 11 or not any(char.isdigit() for char in label)):\n",
    "                        continue\n",
    "                    # Track character counts for balancing\n",
    "                    self.character_counts.update(label)\n",
    "\n",
    "                    # Load and process the image\n",
    "                    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "                    image = self.transform(image)\n",
    "\n",
    "                    # Process label and encode it to fixed length\n",
    "                    encoded_label = encode_to_labels(label, self.char_list, target_len=self.pred_len)\n",
    "                    label_len = min(len(label), self.pred_len)  # Actual length of original label\n",
    "\n",
    "                    self.images.append(image)\n",
    "                    self.labels.append(encoded_label)\n",
    "                    self.label_length.append(label_len)\n",
    "                    self.input_length.append(self.pred_len)\n",
    "                    self.orig_txt.append(label)\n",
    "\n",
    "        # Data augmentation to balance character distribution\n",
    "        if self.augment:\n",
    "            self.balance_dataset()\n",
    "\n",
    "    def balance_dataset(self):\n",
    "        max_count = max(self.character_counts.values())  # Target count for balancing\n",
    "\n",
    "        # Create additional images for underrepresented characters\n",
    "        augmented_images = []\n",
    "        augmented_labels = []\n",
    "        augmented_label_lengths = []\n",
    "        augmented_input_lengths = []\n",
    "        augmented_orig_txts = []\n",
    "\n",
    "        for idx in range(len(self.images)):\n",
    "            label = self.orig_txt[idx]\n",
    "            char_count = Counter(label)\n",
    "\n",
    "            for char in char_count:\n",
    "                # Augment the image multiple times\n",
    "                random_max_count = max_count * (random.randint(50 + random.randint(0, 10), 100 + random.randint(-10, 0)) / 100.0)\n",
    "                while self.character_counts[char] < random_max_count:\n",
    "                    augmented_image = self.augment_image(self.images[idx])\n",
    "                    augmented_images.append(augmented_image)\n",
    "                    augmented_labels.append(self.labels[idx])\n",
    "                    augmented_label_lengths.append(self.label_length[idx])\n",
    "                    augmented_input_lengths.append(self.input_length[idx])\n",
    "                    augmented_orig_txts.append(label)\n",
    "                    self.character_counts[char] += 1\n",
    "\n",
    "        # Append augmented data to the dataset\n",
    "        self.images.extend(augmented_images)\n",
    "        self.labels.extend(augmented_labels)\n",
    "        self.label_length.extend(augmented_label_lengths)\n",
    "        self.input_length.extend(augmented_input_lengths)\n",
    "        self.orig_txt.extend(augmented_orig_txts)\n",
    "\n",
    "    def augment_image(self, image):\n",
    "        # Apply random transformations to augment the image\n",
    "        image_pil = transforms.ToPILImage()(image)\n",
    "        enhancer = ImageEnhance.Brightness(image_pil)\n",
    "        image_pil = enhancer.enhance(1 + random.random() * 0.25)  # Increase brightness\n",
    "        return self.transform(image_pil)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        label_len = self.label_length[idx]\n",
    "        input_len = self.input_length[idx]\n",
    "        orig_txt = self.orig_txt[idx]\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'label_len': torch.tensor(label_len, dtype=torch.long),\n",
    "            'input_len': torch.tensor(input_len, dtype=torch.long),\n",
    "            'orig_txt': orig_txt\n",
    "        }\n",
    "\n",
    "    \n",
    "    def plot_character_distribution(self):\n",
    "        # Plot a graph of character distribution\n",
    "        char_counts = dict(self.character_counts)\n",
    "        \n",
    "        # Sort characters and counts based on the character keys\n",
    "        sorted_characters = sorted(char_counts.keys())\n",
    "        sorted_counts = [char_counts[char] for char in sorted_characters]\n",
    "        \n",
    "        # Plot the distribution\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(sorted_characters, sorted_counts, marker='o', linestyle='-', color='b')\n",
    "        plt.xlabel('Characters')\n",
    "        plt.ylabel('Counts')\n",
    "        plt.title('Character Distribution')\n",
    "        plt.grid()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1733641332284,
     "user": {
      "displayName": "Sanele Hlabisa",
      "userId": "02109846400367384167"
     },
     "user_tz": -120
    },
    "id": "_ngqGJLmBhrV",
    "papermill": {
     "duration": 0.020097,
     "end_time": "2025-09-03T13:26:25.139657",
     "exception": false,
     "start_time": "2025-09-03T13:26:25.119560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CRNN Model Architecture\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, img_channel, img_height, img_width, num_class,\n",
    "\n",
    "                 map_to_seq_hidden=64, rnn_hidden=256, leaky_relu=False):\n",
    "\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "\n",
    "\n",
    "        self.cnn, (output_channel, output_height, output_width) = self._cnn_backbone(img_channel, img_height, img_width, leaky_relu)\n",
    "\n",
    "\n",
    "\n",
    "        self.map_to_seq = nn.Linear(output_channel * output_height, map_to_seq_hidden)\n",
    "\n",
    "\n",
    "\n",
    "        self.rnn1 = nn.LSTM(map_to_seq_hidden, rnn_hidden, bidirectional=True)\n",
    "\n",
    "        self.rnn2 = nn.LSTM(2 * rnn_hidden, rnn_hidden, bidirectional=True)\n",
    "\n",
    "\n",
    "\n",
    "        self.dense = nn.Linear(2 * rnn_hidden, num_class)\n",
    "\n",
    "\n",
    "\n",
    "    def _cnn_backbone(self, img_channel, img_height, img_width, leaky_relu):\n",
    "\n",
    "        assert img_height % 16 == 0\n",
    "\n",
    "        assert img_width % 4 == 0\n",
    "\n",
    "\n",
    "\n",
    "        channels = [img_channel, 64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        kernel_sizes = [3, 3, 3, 3, 3, 3, 2]\n",
    "\n",
    "        strides = [1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "        paddings = [1, 1, 1, 1, 1, 1, 0]\n",
    "\n",
    "\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "\n",
    "\n",
    "        def conv_relu(i, batch_norm=False):\n",
    "\n",
    "            # shape of input: (batch, input_channel, height, width)\n",
    "\n",
    "            input_channel = channels[i]\n",
    "\n",
    "            output_channel = channels[i+1]\n",
    "\n",
    "\n",
    "\n",
    "            cnn.add_module(\n",
    "\n",
    "                f'conv{i}',\n",
    "\n",
    "                nn.Conv2d(input_channel, output_channel, kernel_sizes[i], strides[i], paddings[i])\n",
    "\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "            if batch_norm:\n",
    "\n",
    "                cnn.add_module(f'batchnorm{i}', nn.BatchNorm2d(output_channel))\n",
    "\n",
    "\n",
    "\n",
    "            relu = nn.LeakyReLU(0.2, inplace=True) if leaky_relu else nn.ReLU(inplace=True)\n",
    "\n",
    "            cnn.add_module(f'relu{i}', relu)\n",
    "\n",
    "\n",
    "\n",
    "        # size of image: (channel, height, width) = (img_channel, img_height, img_width)\n",
    "\n",
    "        conv_relu(0)\n",
    "\n",
    "        cnn.add_module('pooling0', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # (64, img_height // 2, img_width // 2)\n",
    "\n",
    "\n",
    "\n",
    "        conv_relu(1)\n",
    "\n",
    "        cnn.add_module('pooling1', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # (128, img_height // 4, img_width // 4)\n",
    "\n",
    "\n",
    "\n",
    "        conv_relu(2)\n",
    "\n",
    "        conv_relu(3)\n",
    "\n",
    "        cnn.add_module(\n",
    "\n",
    "            'pooling2',\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=(2, 1))\n",
    "\n",
    "        )  # (256, img_height // 8, img_width // 4)\n",
    "\n",
    "\n",
    "\n",
    "        conv_relu(4, batch_norm=True)\n",
    "\n",
    "        conv_relu(5, batch_norm=True)\n",
    "\n",
    "        cnn.add_module(\n",
    "\n",
    "            'pooling3',\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=(2, 1))\n",
    "\n",
    "        )  # (512, img_height // 16, img_width // 4)\n",
    "\n",
    "\n",
    "\n",
    "        conv_relu(6)  # (512, img_height // 16 - 1, img_width // 4 - 1)\n",
    "\n",
    "\n",
    "\n",
    "        output_channel, output_height, output_width = channels[-1], img_height // 16 - 1, img_width // 4 - 1\n",
    "\n",
    "        return cnn, (output_channel, output_height, output_width)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "\n",
    "        # shape of images: (batch, channel, height, width)\n",
    "\n",
    "\n",
    "\n",
    "        conv = self.cnn(images)\n",
    "\n",
    "        batch, channel, height, width = conv.size()\n",
    "\n",
    "\n",
    "\n",
    "        conv = conv.view(batch, channel * height, width)\n",
    "\n",
    "        conv = conv.permute(2, 0, 1)  # (width, batch, feature)\n",
    "\n",
    "        seq = self.map_to_seq(conv)\n",
    "\n",
    "\n",
    "\n",
    "        recurrent, _ = self.rnn1(seq)\n",
    "\n",
    "        recurrent, _ = self.rnn2(recurrent)\n",
    "\n",
    "\n",
    "\n",
    "        output = self.dense(recurrent)\n",
    "\n",
    "        return output  # shape: (seq_len, batch, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1733641332285,
     "user": {
      "displayName": "Sanele Hlabisa",
      "userId": "02109846400367384167"
     },
     "user_tz": -120
    },
    "id": "6OpZ7Z_G6Lyg",
    "papermill": {
     "duration": 0.00504,
     "end_time": "2025-09-03T13:26:25.150006",
     "exception": false,
     "start_time": "2025-09-03T13:26:25.144966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "papermill": {
     "duration": 0.019505,
     "end_time": "2025-09-03T13:26:25.174725",
     "exception": false,
     "start_time": "2025-09-03T13:26:25.155220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModifiedCRNN1(nn.Module):\n",
    "\n",
    "    def __init__(self, img_channel, img_height, img_width, num_class,\n",
    "\n",
    "                 map_to_seq_hidden=64, rnn_hidden=256, leaky_relu=False):\n",
    "\n",
    "        super(ModifiedCRNN1, self).__init__()\n",
    "\n",
    "\n",
    "\n",
    "        self.cnn, (output_channel, output_height, output_width) = self._cnn_backbone(img_channel, img_height, img_width, leaky_relu)\n",
    "\n",
    "\n",
    "\n",
    "        self.map_to_seq = nn.Linear(output_channel * output_height, map_to_seq_hidden)\n",
    "\n",
    "\n",
    "\n",
    "        self.rnn1 = nn.LSTM(map_to_seq_hidden, rnn_hidden, bidirectional=True)\n",
    "\n",
    "        self.dense = nn.Linear(2 * rnn_hidden, num_class)\n",
    "\n",
    "\n",
    "\n",
    "    def _cnn_backbone(self, img_channel, img_height, img_width, leaky_relu):\n",
    "\n",
    "        assert img_height % 16 == 0\n",
    "\n",
    "        assert img_width % 4 == 0\n",
    "\n",
    "\n",
    "\n",
    "        channels = [img_channel, 64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        kernel_sizes = [3, 3, 3, 3, 3, 3, 2]\n",
    "\n",
    "        strides = [1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "        paddings = [1, 1, 1, 1, 1, 1, 0]\n",
    "\n",
    "\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "\n",
    "\n",
    "        def conv_relu(i, batch_norm=False):\n",
    "\n",
    "            input_channel = channels[i]\n",
    "\n",
    "            output_channel = channels[i+1]\n",
    "\n",
    "\n",
    "\n",
    "            cnn.add_module(\n",
    "\n",
    "                f'conv{i}',\n",
    "\n",
    "                nn.Conv2d(input_channel, output_channel, kernel_sizes[i], strides[i], paddings[i])\n",
    "\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "            if batch_norm:\n",
    "\n",
    "                cnn.add_module(f'batchnorm{i}', nn.BatchNorm2d(output_channel))\n",
    "\n",
    "\n",
    "\n",
    "            relu = nn.LeakyReLU(0.2, inplace=True) if leaky_relu else nn.ReLU(inplace=True)\n",
    "\n",
    "            cnn.add_module(f'relu{i}', relu)\n",
    "\n",
    "\n",
    "\n",
    "        conv_relu(0)\n",
    "\n",
    "        cnn.add_module('pooling0', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        conv_relu(1)\n",
    "\n",
    "        cnn.add_module('pooling1', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        conv_relu(2)\n",
    "\n",
    "        conv_relu(3)\n",
    "\n",
    "        cnn.add_module('pooling2', nn.MaxPool2d(kernel_size=(2, 1)))\n",
    "\n",
    "        conv_relu(4, batch_norm=True)\n",
    "\n",
    "        conv_relu(5, batch_norm=True)\n",
    "\n",
    "        cnn.add_module('pooling3', nn.MaxPool2d(kernel_size=(2, 1)))\n",
    "\n",
    "        conv_relu(6)\n",
    "\n",
    "\n",
    "\n",
    "        output_channel, output_height, output_width = channels[-1], img_height // 16 - 1, img_width // 4 - 1\n",
    "\n",
    "        return cnn, (output_channel, output_height, output_width)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "\n",
    "      # shape of images: (batch, channel, height, width)\n",
    "\n",
    "      conv = self.cnn(images)\n",
    "\n",
    "      batch, channel, height, width = conv.size()\n",
    "\n",
    "      #print(f\"Conv shape: {conv.shape}\")  # Debugging output\n",
    "\n",
    "\n",
    "\n",
    "      conv = conv.view(batch, channel * height, width)\n",
    "\n",
    "      #print(f\"Shape before map_to_seq: {conv.shape}\")  # Debugging output\n",
    "\n",
    "\n",
    "\n",
    "      conv = conv.permute(2, 0, 1)  # (width, batch, feature)\n",
    "\n",
    "      #print(f\"Shape after permute: {conv.shape}\")  # Debugging output\n",
    "\n",
    "\n",
    "\n",
    "      seq = self.map_to_seq(conv)\n",
    "\n",
    "      #print(f\"Shape after map_to_seq: {seq.shape}\")  # Debugging output\n",
    "\n",
    "\n",
    "\n",
    "      recurrent, _ = self.rnn1(seq)\n",
    "\n",
    "      #print(f\"Shape after rnn1: {recurrent.shape}\")  # Debugging output\n",
    "\n",
    "\n",
    "\n",
    "      output = self.dense(recurrent)\n",
    "\n",
    "      #print(f\"Shape after dense: {output.shape}\")  # Debugging output\n",
    "\n",
    "\n",
    "\n",
    "      return output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "papermill": {
     "duration": 0.008195,
     "end_time": "2025-09-03T13:26:25.188959",
     "exception": false,
     "start_time": "2025-09-03T13:26:25.180764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "papermill": {
     "duration": 0.023335,
     "end_time": "2025-09-03T13:26:25.219193",
     "exception": false,
     "start_time": "2025-09-03T13:26:25.195858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModifiedCRNN2(nn.Module):\n",
    "    def __init__(self, img_channel, img_height, img_width, num_class,\n",
    "                 map_to_seq_hidden=64, rnn_hidden=256, leaky_relu=False):\n",
    "        super(ModifiedCRNN2, self).__init__()\n",
    "\n",
    "        self.cnn, (output_channel, output_height, output_width) = self._cnn_backbone(img_channel, img_height, img_width, leaky_relu)\n",
    "\n",
    "        # Adjust the input size to map_to_seq based on the CNN output\n",
    "        self.map_to_seq = nn.Linear(output_channel * output_height, map_to_seq_hidden)\n",
    "\n",
    "        self.rnn1 = nn.LSTM(map_to_seq_hidden, rnn_hidden, bidirectional=True)\n",
    "        self.rnn2 = nn.LSTM(2 * rnn_hidden, rnn_hidden, bidirectional=True)\n",
    "\n",
    "        self.dense = nn.Linear(2 * rnn_hidden, num_class)\n",
    "\n",
    "    def _cnn_backbone(self, img_channel, img_height, img_width, leaky_relu):\n",
    "        assert img_height % 16 == 0\n",
    "        assert img_width % 4 == 0\n",
    "\n",
    "        channels = [img_channel, 64, 128, 256, 512, 1024]\n",
    "        kernel_sizes = [3] * len(channels[:-1])\n",
    "        strides = [1] * len(channels[:-1])\n",
    "        paddings = [1] * len(channels[:-1])\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def conv_relu(i, batch_norm=False):\n",
    "            input_channel = channels[i]\n",
    "            output_channel = channels[i + 1]\n",
    "\n",
    "            cnn.add_module(f'conv{i}', nn.Conv2d(input_channel, output_channel, kernel_sizes[i], strides[i], paddings[i]))\n",
    "\n",
    "            if batch_norm:\n",
    "                cnn.add_module(f'batchnorm{i}', nn.BatchNorm2d(output_channel))\n",
    "\n",
    "            relu = nn.LeakyReLU(0.2, inplace=True) if leaky_relu else nn.ReLU(inplace=True)\n",
    "            cnn.add_module(f'relu{i}', relu)\n",
    "\n",
    "        # Build the CNN layers\n",
    "        conv_relu(0)\n",
    "        cnn.add_module('pooling0', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        conv_relu(1)\n",
    "        cnn.add_module('pooling1', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        conv_relu(2)\n",
    "        conv_relu(3)\n",
    "        cnn.add_module('pooling2', nn.MaxPool2d(kernel_size=(2, 1)))\n",
    "\n",
    "        conv_relu(4, batch_norm=True)\n",
    "        cnn.add_module('pooling3', nn.MaxPool2d(kernel_size=(2, 1)))\n",
    "\n",
    "        output_channel = channels[-1]\n",
    "        output_height = img_height // 16  # Adjust this based on pooling\n",
    "        output_width = img_width // 4  # Adjust this based on pooling\n",
    "\n",
    "        return cnn, (output_channel, output_height, output_width)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # shape of images: (batch, channel, height, width)\n",
    "        conv = self.cnn(images)\n",
    "        batch, channel, height, width = conv.size()\n",
    "\n",
    "        # Reshape the output for LSTM\n",
    "        conv = conv.view(batch, channel * height, width)\n",
    "        conv = conv.permute(2, 0, 1)  # (width, batch, features)\n",
    "\n",
    "        # Pass through the mapping layer\n",
    "        seq = self.map_to_seq(conv)\n",
    "\n",
    "        recurrent, _ = self.rnn1(seq)\n",
    "        recurrent, _ = self.rnn2(recurrent)\n",
    "\n",
    "        output = self.dense(recurrent)\n",
    "\n",
    "        return output  # shape: (seq_len, batch, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "papermill": {
     "duration": 0.009109,
     "end_time": "2025-09-03T13:26:25.235140",
     "exception": false,
     "start_time": "2025-09-03T13:26:25.226031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1733641448928,
     "user": {
      "displayName": "Sanele Hlabisa",
      "userId": "02109846400367384167"
     },
     "user_tz": -120
    },
    "id": "enwxMIAUdksn",
    "papermill": {
     "duration": 0.036239,
     "end_time": "2025-09-03T13:26:25.278937",
     "exception": false,
     "start_time": "2025-09-03T13:26:25.242698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Updated Training function to use test dataset for final evaluation\n",
    "def train(model, train_loader, val_loader, test_loader, num_epochs, initial_lr, char_list, model_save_path, device):\n",
    "    ctc_loss = nn.CTCLoss(blank=len(char_list) - 1, zero_infinity=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5)\n",
    "\n",
    "    # Resume training if a saved model exists\n",
    "    start_epoch = 0\n",
    "    best_val_accuracy = 0.0\n",
    "    if os.path.exists(model_save_path):\n",
    "        print(f\"Resuming training from {model_save_path}...\")\n",
    "        checkpoint = torch.load(model_save_path, weights_only=True)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_accuracy = checkpoint['best_val_accuracy']\n",
    "        print(f\"Resumed from epoch {start_epoch}, best val accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            images = batch['image'].to(device)  # Move images to device\n",
    "            targets = batch['label'].to(device)  # Move targets to device\n",
    "            target_lengths = batch['label_len'].to(device)  # Move target lengths to device\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            output_lengths = torch.full(size=(outputs.size(1),), fill_value=outputs.size(0), dtype=torch.long).to(device)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = ctc_loss(outputs.log_softmax(2), targets, output_lengths, target_lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss, val_accuracy = validate_model(model, val_loader, ctc_loss, char_list, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        # Adjust learning rate based on validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save the best model based on validation accuracy\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            # Append accuracy to the model save path\n",
    "            model_filename = f\"{model_save_path.split('.pth')[0]}_acc_{val_accuracy:.4f}.pth\"\n",
    "            save_model(model, optimizer, scheduler, epoch, val_accuracy, model_filename)\n",
    "            save_model(model, optimizer, scheduler, epoch, val_accuracy, model_save_path)\n",
    "            print(f\"Saved the best model at epoch {epoch + 1} with val accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val Accuracy: {val_accuracy:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # Evaluate on the test dataset after training\n",
    "    test_loss, test_accuracy = validate_model(model, test_loader, ctc_loss, char_list, device)\n",
    "    print(f\"Final Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses, val_accuracies\n",
    "\n",
    "# Validation function remains unchanged\n",
    "def validate_model(model, val_loader, ctc_loss, char_list, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_characters = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['image'].to(device)  # Move images to device\n",
    "            targets = batch['label'].to(device)  # Move targets to device\n",
    "            target_lengths = batch['label_len'].to(device)  # Move target lengths to device\n",
    "\n",
    "            outputs = model(images)\n",
    "            output_lengths = torch.full(size=(outputs.size(1),), fill_value=outputs.size(0), dtype=torch.long).to(device)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = ctc_loss(outputs.log_softmax(2), targets, output_lengths, target_lengths)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Decode predictions and compare with targets\n",
    "            predictions = decode_predictions(outputs, char_list)\n",
    "            ground_truths = decode_ground_truths(targets, char_list)\n",
    "\n",
    "            for pred, gt in zip(predictions, ground_truths):\n",
    "                total_correct += sum(p == g for p, g in zip(pred, gt))\n",
    "                total_characters += len(gt)\n",
    "\n",
    "    val_accuracy = total_correct / total_characters if total_characters > 0 else 0.0\n",
    "    return val_loss / len(val_loader), val_accuracy\n",
    "\n",
    "# Save model function\n",
    "def save_model(model, optimizer, scheduler, epoch, best_val_accuracy, path):\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'best_val_accuracy': best_val_accuracy\n",
    "    }, path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "# Load model function\n",
    "def load_model(model, path):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path, weights_only=True)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        print(f\"Model loaded from {path}\")\n",
    "        return checkpoint\n",
    "    else:\n",
    "        print(f\"No model found at {path}\")\n",
    "        return None\n",
    "\n",
    "# Decode predictions using CTC\n",
    "def decode_predictions(outputs, char_list):\n",
    "    outputs = outputs.softmax(2).argmax(2).transpose(1, 0).cpu().numpy()\n",
    "    decoded = []\n",
    "    for sequence in outputs:\n",
    "        chars = []\n",
    "        prev_char = None\n",
    "        for c in sequence:\n",
    "            if c != prev_char and c != len(char_list) - 1:  # Avoid duplicates and blank token\n",
    "                chars.append(char_list[c])\n",
    "            prev_char = c\n",
    "        decoded.append(''.join(chars))\n",
    "    return decoded\n",
    "\n",
    "# Decode ground truths\n",
    "def decode_ground_truths(targets, char_list):\n",
    "    decoded = []\n",
    "    for sequence in targets:\n",
    "        chars = [char_list[c] for c in sequence if c != len(char_list) - 1]\n",
    "        decoded.append(''.join(chars))\n",
    "    return decoded\n",
    "\n",
    "# Visualize predictions for a few random test data points\n",
    "def visualize_predictions(model, test_loader, char_list, device, num_samples=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Randomly pick a few samples from the test_loader\n",
    "        test_samples = random.sample(list(test_loader.dataset), num_samples)\n",
    "        for sample in test_samples:\n",
    "            image = sample['image'].unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "            target = sample['label'].to(device)  # Move target to device\n",
    "            \n",
    "            output = model(image)\n",
    "            prediction = decode_predictions(output, char_list)[0]\n",
    "            ground_truth = decode_ground_truths(target.unsqueeze(0), char_list)[0]  # Decode ground truth\n",
    "            \n",
    "            correct_chars = sum(p == g for p, g in zip(prediction, ground_truth))\n",
    "            total_chars = len(ground_truth)\n",
    "\n",
    "            plt.imshow(image.squeeze(0).squeeze(0).cpu().numpy(), cmap='gray')\n",
    "            plt.title(f'Prediction: {prediction}\\nGround Truth: {ground_truth}\\n'\n",
    "                      f'Correct Characters: {correct_chars}/{total_chars}')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "# Evaluate model performance on the test dataset\n",
    "def evaluate_model(model, test_loader, char_list, device):\n",
    "    test_loss, test_accuracy = validate_model(model, test_loader, nn.CTCLoss(blank=len(char_list) - 1, zero_infinity=True), char_list, device)\n",
    "    print(f\"Final Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Main evaluation and visualization flow\n",
    "def main_evaluation_and_visualization(model, val_loader, test_loader, char_list, device):\n",
    "    # Visualize predictions on a few test data points\n",
    "    print(\"Visualizing predictions on test data:\")\n",
    "    visualize_predictions(model, test_loader, char_list, device)\n",
    "\n",
    "    # Evaluate model performance on the test dataset\n",
    "    print(\"Evaluating model performance on test data:\")\n",
    "    evaluate_model(model, test_loader, char_list, device)\n",
    "\n",
    "# Visualize predictions with statistics\n",
    "def visualize_predictions_with_stats(model, loader, char_list, device):\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "\n",
    "    total_correct = 0\n",
    "    total_characters = 0\n",
    "    pred_char_counts = Counter()\n",
    "    true_char_counts = Counter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images = batch['image'].to(device)  # Move images to device\n",
    "            targets = batch['label'].to(device)  # Move targets to device\n",
    "\n",
    "            # Generate outputs and decode predictions\n",
    "            outputs = model(images)\n",
    "            predictions = decode_predictions(outputs, char_list)\n",
    "            ground_truths = decode_ground_truths(targets, char_list)\n",
    "\n",
    "            for i in range(len(predictions)):  # Iterate through each example in the batch\n",
    "                pred = predictions[i]\n",
    "                gt = ground_truths[i]\n",
    "\n",
    "                # Count correct characters and total characters\n",
    "                correct_chars = sum(p == g for p, g in zip(pred, gt))\n",
    "                total_correct += correct_chars\n",
    "                total_chars = len(gt)\n",
    "                total_characters += total_chars\n",
    "\n",
    "                # Update character frequency counters\n",
    "                pred_char_counts.update(pred)\n",
    "                true_char_counts.update(gt)\n",
    "\n",
    "                # Display example visualizations\n",
    "                if i < 5:  # Limit to 5 examples for display\n",
    "                    plt.imshow(images[i].squeeze(0).cpu().numpy(), cmap='gray')\n",
    "                    plt.title(f'Prediction: {pred}\\nGround Truth: {gt}\\n'\n",
    "                              f'Correct Characters: {correct_chars}/{total_chars}')\n",
    "                    plt.show()\n",
    "\n",
    "    # Compute and display accuracy\n",
    "    accuracy = total_correct / total_characters if total_characters > 0 else 0.0\n",
    "    processing_time = time.time() - start_time\n",
    "\n",
    "    # Print frequency comparison\n",
    "    print(\"\\nCharacter Frequency Comparison:\")\n",
    "    print(f\"{'Character':<10} {'Predicted':<10} {'True':<10}\")\n",
    "    for char in sorted(set(pred_char_counts.keys()).union(set(true_char_counts.keys()))):\n",
    "        print(f\"{char:<10} {pred_char_counts[char]:<10} {true_char_counts[char]:<10}\")\n",
    "\n",
    "    # Print final accuracy and processing time\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Processing Time: {processing_time:.2f} seconds\")\n",
    "\n",
    "    return accuracy, processing_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1733641332285,
     "user": {
      "displayName": "Sanele Hlabisa",
      "userId": "02109846400367384167"
     },
     "user_tz": -120
    },
    "id": "SyJr926RWt92",
    "papermill": {
     "duration": 0.01387,
     "end_time": "2025-09-03T13:26:25.298376",
     "exception": false,
     "start_time": "2025-09-03T13:26:25.284506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters and settings\n",
    "img_channel = 1  # Grayscale\n",
    "img_height = 32\n",
    "img_width = 352\n",
    "learning_rate = 0.001\n",
    "num_classes = len(char_list)\n",
    "batch_size = 4\n",
    "num_epochs = 512\n",
    "model_save_path = '/kaggle/working/modified_crnn_model/checkpoint.pth'\n",
    "#model_save_path = \"/kaggle/input/modified_ccrn_model/pytorch/default/1/modified_ccrn_model.pth\"\n",
    "data_path = '/kaggle/input/dataset-cc/final_dataset/recognition/images_with_labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "papermill": {
     "duration": 0.006176,
     "end_time": "2025-09-03T13:26:25.313591",
     "exception": false,
     "start_time": "2025-09-03T13:26:25.307415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "id": "62F8gu4_XHUX",
    "outputId": "dc311c1d-2bbd-44be-85cc-9f6082d18a4a",
    "papermill": {
     "duration": 15.107274,
     "end_time": "2025-09-03T13:26:40.427357",
     "exception": false,
     "start_time": "2025-09-03T13:26:25.320083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "char_list = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "             'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', \n",
    "             'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', \n",
    "             'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', \n",
    "             'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', \n",
    "             'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \n",
    "             'y', 'z', ' ']  # Define your character set\n",
    "\n",
    "# Prepare dataset and dataloaders\n",
    "full_dataset = RecogDataset(data_path=data_path, char_list=char_list)\n",
    "#full_dataset = RecogDataset(data_path=data_path, char_list=char_list, target_size=(32, 512), pred_len=11, augment=False)\n",
    "\n",
    "# Plot character distribution\n",
    "full_dataset.plot_character_distribution()\n",
    "\n",
    "# Split the dataset into train (70%), validation (20%), and test (10%)\n",
    "train_indices, temp_indices = train_test_split(\n",
    "    list(range(len(full_dataset))), test_size=0.3, random_state=42\n",
    ")\n",
    "val_indices, test_indices = train_test_split(\n",
    "    temp_indices, test_size=1/3, random_state=42  # 1/3 of 30% is 10%\n",
    ")\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "papermill": {
     "duration": 0.31845,
     "end_time": "2025-09-03T13:26:40.752431",
     "exception": false,
     "start_time": "2025-09-03T13:26:40.433981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "#model = ModifiedCRNN1(img_channel, img_height, img_width, num_classes)\n",
    "model = ModifiedCRNN2(img_channel, img_height, img_width, num_classes)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "papermill": {
     "duration": 16401.36364,
     "end_time": "2025-09-03T18:00:02.122814",
     "exception": false,
     "start_time": "2025-09-03T13:26:40.759174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_accuracies = train(model, train_loader, val_loader, test_loader, num_epochs, learning_rate, char_list, model_save_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "papermill": {
     "duration": 0.321959,
     "end_time": "2025-09-03T18:00:02.474487",
     "exception": false,
     "start_time": "2025-09-03T18:00:02.152528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(train_losses)\n",
    "_ = plt.plot(val_losses)\n",
    "_ = plt.title(\"Train and Validation Loss\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Loss\")\n",
    "_ = plt.legend([\"Training Loss\", \"Validation Loss\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19",
   "metadata": {
    "papermill": {
     "duration": 0.029189,
     "end_time": "2025-09-03T18:00:02.533492",
     "exception": false,
     "start_time": "2025-09-03T18:00:02.504303",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "papermill": {
     "duration": 2.32561,
     "end_time": "2025-09-03T18:00:04.888677",
     "exception": false,
     "start_time": "2025-09-03T18:00:02.563067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage after training\n",
    "main_evaluation_and_visualization(model, val_loader, test_loader, char_list, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "executionInfo": {
     "elapsed": 11342,
     "status": "ok",
     "timestamp": 1733641471174,
     "user": {
      "displayName": "Sanele Hlabisa",
      "userId": "02109846400367384167"
     },
     "user_tz": -120
    },
    "id": "DWzl1pSxeE1f",
    "outputId": "5fc0ea6b-9b89-4561-d16b-87610a389522",
    "papermill": {
     "duration": 0.871512,
     "end_time": "2025-09-03T18:00:05.794517",
     "exception": false,
     "start_time": "2025-09-03T18:00:04.923005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the model (optional)\n",
    "#loaded_model = ModifiedCRNN1(img_channel, img_height, img_width, num_classes)\n",
    "loaded_model = ModifiedCRNN2(img_channel, img_height, img_width, num_classes)\n",
    "loaded_model.to(device)\n",
    "checkpoint = torch.load(model_save_path, weights_only=True)\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# Visualize some predictions \n",
    "visualize_predictions(loaded_model, val_loader, char_list, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1733641369214,
     "user": {
      "displayName": "Sanele Hlabisa",
      "userId": "02109846400367384167"
     },
     "user_tz": -120
    },
    "id": "pZlyPYsR12qr",
    "papermill": {
     "duration": 200.382497,
     "end_time": "2025-09-03T18:03:26.241399",
     "exception": false,
     "start_time": "2025-09-03T18:00:05.858902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize predictions and analyze statistics\n",
    "accuracy, processing_time = visualize_predictions_with_stats(loaded_model, val_loader, char_list, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "papermill": {
     "duration": 1.016349,
     "end_time": "2025-09-03T18:03:28.239565",
     "exception": false,
     "start_time": "2025-09-03T18:03:27.223216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Final Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Model Processing Time: {processing_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6204761,
     "sourceId": 10067463,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 186255,
     "modelInstanceId": 163905,
     "sourceId": 192244,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 186712,
     "modelInstanceId": 164379,
     "sourceId": 192762,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 437924,
     "modelInstanceId": 420286,
     "sourceId": 550319,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 440983,
     "modelInstanceId": 423447,
     "sourceId": 556811,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 440996,
     "modelInstanceId": 423461,
     "sourceId": 556836,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 441348,
     "modelInstanceId": 423832,
     "sourceId": 557559,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 441430,
     "modelInstanceId": 423911,
     "sourceId": 557728,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16634.626351,
   "end_time": "2025-09-03T18:03:31.491733",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-03T13:26:16.865382",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
