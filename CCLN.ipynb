{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9990986,"sourceType":"datasetVersion","datasetId":6148850},{"sourceId":176128,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":149959,"modelId":172454},{"sourceId":593603,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":444236,"modelId":460729},{"sourceId":593680,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":444306,"modelId":460801},{"sourceId":653753,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":493922,"modelId":509316}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Standard Library Imports\nimport os\nimport math\nimport random\nimport xml.etree.ElementTree as ET\n\n# Third-Party Library Imports\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\n\n# PyTorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n# torchvision Imports\nimport torchvision\nfrom torchvision import transforms\nimport torchvision.transforms.functional as TF\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torchvision import models\nfrom torchvision.ops import complete_box_iou_loss\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n","metadata":{"id":"XzE4KR6wBCEm","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:17:38.883461Z","iopub.execute_input":"2025-11-19T20:17:38.883701Z","iopub.status.idle":"2025-11-19T20:18:17.016726Z","shell.execute_reply.started":"2025-11-19T20:17:38.883675Z","shell.execute_reply":"2025-11-19T20:18:17.015770Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LocalizationDataset(Dataset):\n    def __init__(self, dataset_folder, target_size=(256, 256), augment=False):\n        self.target_size = target_size\n        self.augment = augment\n\n        # Will store PIL images or image paths\n        self.images = []      # list of PIL images (augmented) or strings (original paths)\n        self.bboxes = []      # list of 4-dim tensors (normalized)\n\n        image_folder = os.path.join(dataset_folder, \"images\")\n        label_folder = os.path.join(dataset_folder, \"labels\")\n\n        # Base transform\n        self.base_transform = transforms.Compose([\n            transforms.Resize(target_size),\n            transforms.ToTensor(),\n            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n\n        for img_name in os.listdir(image_folder):\n            if not img_name.endswith(\".jpg\"):\n                continue\n\n            img_path = os.path.join(image_folder, img_name)\n            xml_path = os.path.join(label_folder, img_name.replace(\".jpg\", \".xml\"))\n            if not os.path.exists(xml_path):\n                continue\n\n            try:\n                bbox, w, h = self.parse_xml(xml_path)\n\n                nb = [\n                    bbox[0] / w, bbox[1] / h,\n                    bbox[2] / w, bbox[3] / h\n                ]\n\n                # STORE ORIGINAL IMAGE PATH\n                self.images.append(img_path)\n                self.bboxes.append(torch.tensor(nb, dtype=torch.float))\n\n                # AUGMENT IF REQUESTED\n                if augment:\n                    for _ in range(random.randint(0, 3)):\n                        self._add_augmented(img_path, nb)\n\n            except Exception as e:\n                print(f\"Error processing {img_path}: {e}\")\n                continue\n\n    def parse_xml(self, xml_file):\n            \"\"\"Parse the XML file to extract bounding box coordinates and image dimensions.\"\"\"\n            tree = ET.parse(xml_file)\n            root = tree.getroot()\n    \n            # Extract image dimensions\n            width = int(root.find('size/width').text)\n            height = int(root.find('size/height').text)\n    \n            # Extract bounding box information (assumes a single object per image)\n            for obj in root.findall('object'):\n                bbox = obj.find('bndbox')\n                xmin = int(bbox.find('xmin').text)\n                ymin = int(bbox.find('ymin').text)\n                xmax = int(bbox.find('xmax').text)\n                ymax = int(bbox.find('ymax').text)\n    \n                return [xmin, ymin, xmax, ymax], width, height\n    \n            raise ValueError(f\"No bounding box found in {xml_file}\")\n    # ---------------------------------------------------------\n    # ADD AUGMENTED DATA INTO THE LISTS\n    # ---------------------------------------------------------\n    def _add_augmented(self, image_path, bbox):\n        img = Image.open(image_path).convert(\"RGB\")\n        bbox = bbox.copy()\n\n        ops = [\n            self._aug_flip_horizontal,\n            self._aug_flip_vertical,\n            self._aug_color_jitter,\n            self._aug_shift_xy,\n            self._aug_small_rotate\n        ]\n\n        chosen_ops = random.sample(ops, random.randint(1, 3))\n\n        for op in chosen_ops:\n            img, bbox = op(img, bbox)\n\n        # Store augmented image AS A PIL IMAGE (not a path!)\n        self.images.append(img)\n        self.bboxes.append(torch.tensor(bbox, dtype=torch.float))\n\n\n    # ---------------------------------------------------------\n    # AUGMENTATION OPERATIONS\n    # ---------------------------------------------------------\n    def _aug_flip_horizontal(self, img, bbox):\n        img = TF.hflip(img)\n        x1, y1, x2, y2 = bbox\n        return img, [1-x2, y1, 1-x1, y2]\n\n    def _aug_flip_vertical(self, img, bbox):\n        img = TF.vflip(img)\n        x1, y1, x2, y2 = bbox\n        return img, [x1, 1-y2, x2, 1-y1]\n\n    def _aug_color_jitter(self, img, bbox):\n        jitter = transforms.ColorJitter(\n            brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05\n        )\n        return jitter(img), bbox\n\n    def _aug_shift_xy(self, img, bbox):\n        w, h = img.size\n        max_dx, max_dy = int(w * 0.1), int(h * 0.1)\n\n        dx = random.randint(-max_dx, max_dx)\n        dy = random.randint(-max_dy, max_dy)\n\n        img = TF.affine(img, angle=0, translate=(dx, dy), scale=1.0, shear=0)\n\n        x1, y1, x2, y2 = bbox\n        x1 += dx / w; x2 += dx / w\n        y1 += dy / h; y2 += dy / h\n\n        x1 = max(0, min(1, x1))\n        x2 = max(0, min(1, x2))\n        y1 = max(0, min(1, y1))\n        y2 = max(0, min(1, y2))\n\n        return img, [x1, y1, x2, y2]\n\n    def _aug_small_rotate(self, img, bbox):\n        angle = random.uniform(-10, 10)\n        w, h = img.size\n        x1, y1, x2, y2 = bbox\n\n        mask = Image.new(\"L\", (w, h), 0)\n        draw = ImageDraw.Draw(mask)\n        draw.rectangle([x1*w, y1*h, x2*w, y2*h], fill=255)\n\n        img = TF.rotate(img, angle)\n        mask = TF.rotate(mask, angle)\n\n        mask_np = torch.tensor(mask, dtype=torch.uint8)\n        ys, xs = torch.where(mask_np > 0)\n\n        if len(xs) == 0:\n            return img, bbox\n\n        nx1 = xs.min().item() / w\n        nx2 = xs.max().item() / w\n        ny1 = ys.min().item() / h\n        ny2 = ys.max().item() / h\n\n        return img, [nx1, ny1, nx2, ny2]\n\n\n    # ---------------------------------------------------------\n    # __getitem__\n    # ---------------------------------------------------------\n    def __getitem__(self, idx):\n        item = self.images[idx]\n\n        # item is either a path (string) OR a PIL image (augmentation)\n        if isinstance(item, str):\n            img = Image.open(item).convert(\"RGB\")\n        else:\n            img = item\n\n        img = self.base_transform(img)\n        bbox = self.bboxes[idx]\n\n        return {\"image\": img, \"bbox\": bbox}\n\n\n    def __len__(self):\n        return len(self.images)\n","metadata":{"id":"WetMpvSuB5z8","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:18:17.017928Z","iopub.execute_input":"2025-11-19T20:18:17.018368Z","iopub.status.idle":"2025-11-19T20:18:17.037765Z","shell.execute_reply.started":"2025-11-19T20:18:17.018336Z","shell.execute_reply":"2025-11-19T20:18:17.037011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport torch\nimport os \n\n# IMPORTANT: Use the correct path for your Kaggle input\nKAGGLE_RESNET_PATH = \"/kaggle/input/resnet_50/pytorch/default/1/resnet50-11ad3fa6.pth\"\n\nclass CCLN(nn.Module):\n    def __init__(self, pretrained_weights_path=KAGGLE_RESNET_PATH):\n        super(CCLN, self).__init__()\n\n        # ... (Backbone Loading remains the same) ...\n        resnet50 = models.resnet50(weights=None) \n        if pretrained_weights_path and os.path.exists(pretrained_weights_path):\n             state_dict = torch.load(pretrained_weights_path, map_location='cpu', weights_only=True)\n             resnet50.load_state_dict(state_dict, strict=True)\n\n        # --- Model Components (Backbone) ---\n        self.conv1 = resnet50.conv1; self.bn1 = resnet50.bn1; self.relu = resnet50.relu; self.maxpool = resnet50.maxpool\n        self.layer1 = resnet50.layer1; self.layer2 = resnet50.layer2; self.layer3 = resnet50.layer3; self.layer4 = resnet50.layer4     \n        \n        # --- Activations ---\n        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n\n        # ... (Decoder Layers remain the same) ...\n        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv_concat1 = nn.Conv2d(2048 + 1024, 512, kernel_size=1); self.bn_concat1 = nn.BatchNorm2d(512)\n        self.conv_up1 = nn.Conv2d(512, 512, kernel_size=3, padding=1); self.bn_up1 = nn.BatchNorm2d(512)\n\n        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv_concat2 = nn.Conv2d(512 + 512, 256, kernel_size=1); self.bn_concat2 = nn.BatchNorm2d(256)\n        self.conv_up2 = nn.Conv2d(256, 256, kernel_size=3, padding=1); self.bn_up2 = nn.BatchNorm2d(256)\n\n        self.upsample3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv_concat3 = nn.Conv2d(256 + 256, 128, kernel_size=1); self.bn_concat3 = nn.BatchNorm2d(128)\n        self.conv_up3 = nn.Conv2d(128, 128, kernel_size=3, padding=1); self.bn_up3 = nn.BatchNorm2d(128)\n\n        # --- FINAL REGRESSION HEAD (CONV-BASED, SMALLER, AND STABLE) ---\n        # 1. Final 1x1 Conv to predict 4 channels (x_min, y_min, x_max, y_max) per pixel\n        self.conv_final = nn.Conv2d(128, 4, kernel_size=1)\n        nn.init.constant_(self.conv_final.bias, 0.0)\n\n\n    def forward(self, x):\n        # ... (Backbone and Decoder layers are unchanged) ...\n        x = self.conv1(x); x = self.bn1(x); x = self.relu(x); r0 = self.maxpool(x)\n        r1 = self.layer1(r0); r2 = self.layer2(r1); r3 = self.layer3(r2); r4 = self.layer4(r3)\n\n        # Decoder Path (Leaky ReLU)\n        x = self.upsample1(r4); x = torch.cat([x, r3], dim=1); x = self.conv_concat1(x); x = self.leaky_relu(self.bn_concat1(x))\n        x = self.conv_up1(x); x = self.leaky_relu(self.bn_up1(x))\n\n        x = self.upsample2(x); x = torch.cat([x, r2], dim=1); x = self.conv_concat2(x); x = self.leaky_relu(self.bn_concat2(x))\n        x = self.conv_up2(x); x = self.leaky_relu(self.bn_up2(x))\n\n        x = self.upsample3(x); x = torch.cat([x, r1], dim=1); x = self.conv_concat3(x); x = self.leaky_relu(self.bn_concat3(x))\n        x = self.conv_up3(x); x = self.leaky_relu(self.bn_up3(x)) # x is (B, 128, H/4, W/4)\n\n        # --- FINAL REGRESSION HEAD (CONV-BASED) ---\n        # 1. Final Conv (B, 4, H/4, W/4)\n        x = self.conv_final(x)\n        \n        # 2. Sigmoid on the predicted values\n        x = torch.sigmoid(x) \n\n        # 3. Global average pooling to get the final (xmin, ymin, xmax, ymax)\n        output = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:18:17.039552Z","iopub.execute_input":"2025-11-19T20:18:17.039812Z","iopub.status.idle":"2025-11-19T20:18:17.053628Z","shell.execute_reply.started":"2025-11-19T20:18:17.039786Z","shell.execute_reply":"2025-11-19T20:18:17.052971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CCLN3(nn.Module):\n    def __init__(self, weights=True):\n        super(CCLN3, self).__init__()\n\n        # 1. Load the model architecture\n        resnet = resnet50(weights=None)  # don't trigger download\n        \n        # 2. Load state_dict from your uploaded file\n        state_dict_path = '/kaggle/input/resnet_50/pytorch/default/1/resnet50-11ad3fa6.pth'\n        state_dict = torch.load(state_dict_path, map_location='cpu', weights_only=True)\n        \n        # 3. Load weights into the model\n        resnet.load_state_dict(state_dict)\n\n        # Use all layers except the final fully connected layer\n        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # Extract up to the last conv block\n\n        # Upsampling and concatenation layers\n        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv_up1 = nn.Conv2d(2048, 1024, kernel_size=3, padding=1)\n\n        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv_up2 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n\n        self.upsample3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv_up3 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n\n        # Final output layer (4 bounding box coordinates)\n        self.conv_out = nn.Conv2d(256, 4, kernel_size=1)\n\n    def forward(self, x):\n        # Feature extraction using ResNet-50 backbone\n        x = self.backbone(x)  # Output: [batch_size, 2048, 10, 10] for 320x320 input\n\n        # Upsampling layers\n        x = self.upsample1(x)\n        x = F.relu(self.conv_up1(x))\n\n        x = self.upsample2(x)\n        x = F.relu(self.conv_up2(x))\n\n        x = self.upsample3(x)\n        x = F.relu(self.conv_up3(x))\n\n        # Output layer (bounding box coordinates)\n        x = torch.sigmoid(self.conv_out(x))  # Normalize to [0, 1]\n\n        # Flatten the output to [batch_size, 4]\n        x = F.adaptive_avg_pool2d(x, (1, 1))  # Global average pooling to (1, 1)\n        x = x.view(x.size(0), -1)  # Flatten to [batch_size, 4]\n\n        return x\n","metadata":{"id":"6HcD8ZeuqqlZ","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:18:17.054735Z","iopub.execute_input":"2025-11-19T20:18:17.055057Z","iopub.status.idle":"2025-11-19T20:18:17.068499Z","shell.execute_reply.started":"2025-11-19T20:18:17.055020Z","shell.execute_reply":"2025-11-19T20:18:17.067613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_image_with_boxes(image, true_box, pred_box, iou_loss):\n    print(\"Prediction:\", pred_box)\n    print(\"Label:\", true_box)\n    print(\"IoU:\", torchvision.ops.box_iou(true_box, pred_box).item())\n\n    # Convert the PyTorch tensor to NumPy and reshape (H, W, C)\n    image = image.permute(1, 2, 0).cpu().numpy()  # Convert to NumPy array and change shape to (H, W, C)\n\n    # Create a figure and axis\n    plt.figure(figsize=(12, 12))\n    plt.imshow(image)\n\n    # Define colors for bounding boxes\n    colors = ['red', 'green']  # Red for true box, Green for predicted box\n\n    # Draw bounding boxes for true and predicted boxes\n    for i, bbox in enumerate([true_box.squeeze(0), pred_box.squeeze(0)]):  # Squeeze to remove extra dimensions\n        # Ensure bbox is a tensor and convert to float for safety\n        bbox = bbox.cpu().detach().numpy()  # Move to CPU, detach from computation graph, and convert to NumPy\n        bbox = [float(coord) for coord in bbox]  # Convert to float\n\n        x_min, y_min, x_max, y_max = bbox\n\n        # Scale to image dimensions\n        x_min = int(x_min * image.shape[1])  # Scale to image width\n        y_min = int(y_min * image.shape[0])  # Scale to image height\n        x_max = int(x_max * image.shape[1])  # Scale to image width\n        y_max = int(y_max * image.shape[0])  # Scale to image height\n\n        # Draw the rectangle using Matplotlib\n        plt.gca().add_patch(plt.Rectangle(\n            (x_min, y_min),\n            x_max - x_min,\n            y_max - y_min,\n            edgecolor=colors[i],\n            facecolor='none',\n            linewidth=2,\n            label='True Box' if i == 0 else 'Predicted Box'\n        ))\n\n    # Add IoU loss text at the bottom right corner\n    plt.text(\n        image.shape[1] - 200, image.shape[0] - 50,\n        f\"IoU Loss: {iou_loss.item():.4f}\",\n        color='white',\n        fontsize=14,\n        bbox=dict(facecolor='black', alpha=0.8, edgecolor='none', pad=6)\n    )\n\n    # Add legend and hide axes\n    plt.legend()\n    plt.axis('off')\n    plt.show()","metadata":{"id":"Dys269vrNXTs","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:18:17.069834Z","iopub.execute_input":"2025-11-19T20:18:17.070081Z","iopub.status.idle":"2025-11-19T20:18:17.081202Z","shell.execute_reply.started":"2025-11-19T20:18:17.070056Z","shell.execute_reply":"2025-11-19T20:18:17.080452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_metrics(pred_boxes, true_boxes, iou_threshold=0.5):\n    ious = torchvision.ops.box_iou(pred_boxes, true_boxes)\n\n    # True Positives, False Positives, False Negatives\n    tp = (ious > iou_threshold).sum().item()\n    fp = (ious <= iou_threshold).sum().item()\n    fn = (true_boxes.shape[0] - tp)\n\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    return precision, recall, f1\n\ndef calculate_average_precision(pred_boxes, true_boxes):\n    # Flatten for average precision computation\n    ious = torchvision.ops.box_iou(pred_boxes, true_boxes).cpu().numpy().flatten()\n\n    # Binary classification for IoU > threshold (positive), otherwise (negative)\n    labels = (ious > 0.5).astype(int)\n\n    # Use precision_recall_curve and average_precision_score from sklearn\n    precision, recall, _ = precision_recall_curve(labels, ious)\n    average_precision = average_precision_score(labels, ious)\n\n    return precision, recall, average_precision\n\ndef plot_precision_recall_curve(precision, recall, average_precision):\n    plt.figure()\n    plt.plot(recall, precision, label=f'Average Precision = {average_precision:.2f}')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    plt.legend()\n    plt.show()\n\ndef compute_accuracy(predictions, targets, threshold=0.5):\n    \"\"\"\n    Computes the accuracy of the predictions based on IoU.\n\n    Args:\n        predictions: Predicted bounding boxes (tensor of shape [N, 4]).\n        targets: Ground truth bounding boxes (tensor of shape [N, 4]).\n        threshold: IoU threshold to consider a prediction as correct.\n\n    Returns:\n        accuracy: The proportion of correct predictions based on IoU.\n    \"\"\"\n    # Ensure predictions and targets are in the correct shape\n    if predictions.ndim == 1:\n        predictions = predictions.unsqueeze(0)\n    if targets.ndim == 1:\n        targets = targets.unsqueeze(0)\n\n    # Calculate IoU for each predicted box with the corresponding target box\n    ious = (predictions, targets)\n\n    # Count the number of correct predictions based on the threshold\n    correct_predictions = (ious > threshold).sum().item()\n    total_predictions = predictions.size(0)\n\n    # Calculate accuracy\n    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0.0\n    return accuracy\n\ndef plot_metrics_vs_iou_thresholds(pred_boxes, true_boxes, thresholds=np.linspace(0.5, 0.95, 10)):\n    precisions, recalls, f1_scores, accuracies, average_precisions = [], [], [], [], []\n\n    for iou_thresh in thresholds:\n        precision, recall, f1 = calculate_metrics(pred_boxes, true_boxes, iou_thresh)\n        avg_precision = calculate_average_precision(pred_boxes, true_boxes)[2]\n\n        accuracies.append((precision + recall) / 2)  # Simple accuracy\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1)\n        average_precisions.append(avg_precision)\n\n    # Plot the results\n    plt.figure(figsize=(12, 8))\n    plt.plot(thresholds, precisions, label='Precision')\n    plt.plot(thresholds, recalls, label='Recall')\n    plt.plot(thresholds, f1_scores, label='F1-Score')\n    plt.plot(thresholds, accuracies, label='Accuracy')\n    plt.plot(thresholds, average_precisions, label='Average Precision')\n\n    plt.xlabel('IoU Threshold')\n    plt.ylabel('Score')\n    plt.title('Metrics vs IoU Threshold')\n    plt.legend()\n    plt.show()","metadata":{"id":"obLMSSVzQXM0","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:18:17.082465Z","iopub.execute_input":"2025-11-19T20:18:17.082801Z","iopub.status.idle":"2025-11-19T20:18:17.096728Z","shell.execute_reply.started":"2025-11-19T20:18:17.082761Z","shell.execute_reply":"2025-11-19T20:18:17.095824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, optimizer, device, epochs, checkpoint_path):\n    \"\"\"\n    Trains a localization model, computes IoU-based accuracy, and saves the best model.\n    Prints training loss, validation loss, and accuracy on one line per epoch.\n\n    Args:\n        model: The model to train.\n        train_loader: DataLoader for training data.\n        val_loader: DataLoader for validation data.\n        optimizer: Optimizer for training.\n        device: Device to train on (e.g., 'cuda' or 'cpu').\n        epochs: Number of epochs to train.\n        checkpoint_path: Path to save the best model checkpoint.\n    \"\"\"\n    start_epoch = 0\n    best_val_loss = float('inf')\n    best_model_state = None\n    \n    model.train()\n    for c_path in [checkpoint_path, \"/kaggle/input/ccln_0.75.pth/pytorch/default/1/clnn.pth\", \"/kaggle/input/ccln_0.75.pth/pytorch/default/1/clnn_0.75.pth\"]:\n        if False and os.path.exists(c_path):\n            model, best_val_loss = load_checkpoint(model, c_path, device)\n            optimizer.load_state_dict(torch.load(c_path, weights_only=True)['optimizer_state_dict'])\n            start_epoch = torch.load(c_path, weights_only=True)['epoch'] + 1\n            break\n\n    train_losses = []\n    val_losses = []\n    train_accuracies = []\n    val_accuracies = []\n\n    for epoch in range(start_epoch, epochs):\n        running_loss = 0.0\n\n        # Training loop\n        for batch in train_loader:\n            images = batch['image'].to(device)\n            bboxes = batch['bbox'].to(device)\n\n            optimizer.zero_grad()\n            \n            outputs = model(images)\n\n            # Calculate Complete Box IoU loss\n            #loss = torchvision.ops.complete_box_iou_loss(outputs, bboxes, reduction='mean') + torch.nn.functional.mse_loss(outputs, bboxes).mean()\n            loss = torchvision.ops.complete_box_iou_loss(outputs, bboxes, reduction='mean')\n            \n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        # Average training loss for the epoch\n        train_loss = running_loss / len(train_loader)\n        train_losses.append(train_loss)\n\n        # Validation loss and accuracy\n        val_loss, val_accuracy = evaluate_model(model, val_loader, device)\n        val_losses.append(val_loss)\n\n        # Store IoU accuracy for training\n        train_accuracies.append(torchvision.ops.box_iou(outputs, bboxes).mean().item())\n        val_accuracies.append(val_accuracy)\n\n        #current_lr = optimizer.param_groups[0]['lr']\n        print(f'Epoch [{epoch + 1}/{epochs}] | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Accuracy (IoU): {val_accuracy:.4f}')\n\n        # Save the model if the validation loss is the best so far\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model_state = model.state_dict()\n            save_checkpoint(model, optimizer, checkpoint_path, best_val_loss, epoch)\n            if val_accuracy > 0.75:\n                save_checkpoint(model, optimizer, f\"/kaggle/working/clnn_{val_accuracy:.2f}.pth\", best_val_loss, epoch)\n\n\n        # Update the learning rate\n        scheduler.step(val_loss)\n\n    print(f'Training completed. Best model saved with validation loss: {best_val_loss:.4f}')\n    return best_model_state, epoch, train_losses, val_losses, train_accuracies, val_accuracies\n\ndef evaluate_model(model, data_loader, device):\n    \"\"\"\n    Evaluates the model on a validation dataset and computes IoU-based accuracy and loss.\n\n    Args:\n        model: The model to evaluate.\n        data_loader: DataLoader for validation data.\n        device: Device to evaluate on.\n\n    Returns:\n        val_loss (float): The mean loss over the validation dataset.\n        accuracy (float): The mean IoU accuracy over the validation dataset.\n    \"\"\"\n    model.eval()\n    running_loss = 0.0\n    iou_scores = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            images = batch['image'].to(device)\n            bboxes = batch['bbox'].to(device)\n\n            outputs = model(images)\n\n            # Calculate Complete Box IoU loss\n            loss = torchvision.ops.complete_box_iou_loss(outputs, bboxes, reduction='mean')\n            \n            running_loss += loss.item()\n\n            # Compute IoU for each pair of predicted and ground truth bounding boxes\n            iou_scores.append(torchvision.ops.box_iou(outputs, bboxes).mean().item())\n\n    # Compute mean IoU accuracy and loss over all batches\n    val_loss = running_loss / len(data_loader)\n    val_accuracy = sum(iou_scores) / len(iou_scores)\n    return val_loss, val_accuracy\n\ndef save_checkpoint(model, optimizer, checkpoint_path, best_loss, epoch):\n    \"\"\"\n    Saves the model checkpoint.\n\n    Args:\n        model: The model to save.\n        optimizer: The optimizer state to save.\n        checkpoint_path: Path to save the model checkpoint.\n        best_loss: Best validation loss to include in the saved checkpoint.\n        epoch: Current epoch number.\n    \"\"\"\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'best_loss': best_loss,\n        'epoch': epoch\n    }, checkpoint_path)\n    print(f\"Model saved with Best Validation Loss: {best_loss:.4f}\")\n\ndef load_checkpoint(model, checkpoint_path, device):\n    \"\"\"\n    Loads the model checkpoint.\n\n    Args:\n        model: The model to load the checkpoint into.\n        checkpoint_path: Path to the saved model checkpoint.\n        device: Device to load the model on.\n\n    Returns:\n        model: The loaded model.\n        best_loss: The best validation loss from the checkpoint.\n    \"\"\"\n    print(f\"Loading model from: {checkpoint_path}\")\n    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    best_loss = checkpoint['best_loss']\n    return model, best_loss\n    \ndef plot_metrics(train_accuracies, val_accuracies, train_losses, val_losses, checkpoint_path, epoch):\n    \"\"\"\n    Plots the training and validation accuracy and loss curves.\n\n    Args:\n        train_accuracies: List of training accuracies.\n        val_accuracies: List of validation accuracies.\n        train_losses: List of training losses.\n        val_losses: List of validation losses.\n        checkpoint_path: Path to save the plot.\n        epoch: Current epoch.\n    \"\"\"\n    # Plotting training and validation accuracies\n    plt.figure(figsize=(10, 5))\n    plt.plot(range(1, epoch + 1), train_accuracies, label='Train Accuracy', color='blue')\n    plt.plot(range(1, epoch + 1), val_accuracies, label='Validation Accuracy', color='orange')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid()\n    plt.savefig(os.path.join(checkpoint_path, f'accuracy_plot_epoch_{epoch}.png'))\n    plt.close()\n\n    # Plotting training and validation losses\n    plt.figure(figsize=(10, 5))\n    plt.plot(range(1, epoch + 1), train_losses, label='Train Loss', color='blue')\n    plt.plot(range(1, epoch + 1), val_losses, label='Validation Loss', color='orange')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid()\n    plt.savefig(os.path.join(checkpoint_path, f'loss_plot_epoch_{epoch}.png'))\n    plt.close()\n\ndef plot_top_bottom_images(model, test_loader, device, checkpoint_path):\n    \"\"\"\n    Plots the top 5 and bottom 5 images based on IoU loss.\n\n    Args:\n        model: The trained model.\n        test_loader: DataLoader for the test data.\n        device: Device to use for inference.\n        checkpoint_path: Path to save the plots.\n    \"\"\"\n    model.eval()\n    iou_losses = []\n    images = []\n    bboxes = []\n\n    with torch.no_grad():\n        for batch in test_loader:\n            batch_images = batch['image'].to(device)\n            batch_bboxes = batch['bbox'].to(device)\n\n            outputs = model(batch_images)\n            loss = 1 - torchvision.ops.box_iou(outputs, batch_bboxes).mean()\n            iou_losses.extend(loss.cpu().numpy())\n            images.extend(batch_images.cpu())\n            bboxes.extend(batch_bboxes.cpu())\n\n    # Sort the images based on IoU loss\n    sorted_indices = np.argsort(iou_losses)\n\n    # Plot the top 5 and bottom 5 images\n    fig, axs = plt.subplots(2, 5, figsize=(15, 6))\n\n    # Top 5 images\n    for i in range(5):\n        index = sorted_indices[i]\n        ax = axs[0, i]\n        display_image_with_boxes(images[index], bboxes[index], outputs[index], iou_losses[index])\n        ax.set_title(f\"Top {i+1}, IoU Loss: {iou_losses[index]:.4f}\")\n\n    # Bottom 5 images\n    for i in range(5):\n        index = sorted_indices[-i-1]\n        ax = axs[1, i]\n        display_image_with_boxes(images[index], bboxes[index], outputs[index], iou_losses[index])\n        ax.set_title(f\"Bottom {i+1}, IoU Loss: {iou_losses[index]:.4f}\")\n\n    plt.savefig(os.path.join(checkpoint_path, 'top_bottom_images.png'))\n    plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:18:17.098005Z","iopub.execute_input":"2025-11-19T20:18:17.098311Z","iopub.status.idle":"2025-11-19T20:18:17.119545Z","shell.execute_reply.started":"2025-11-19T20:18:17.098277Z","shell.execute_reply":"2025-11-19T20:18:17.118743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameters and settings\nimg_channel = 3  # RGB\nimg_height = 256\nimg_width = 256\nlearning_rate = 0.001\nbatch_size = 16\nnum_epochs = 512\nmodel_save_path = \"/kaggle/working/clnn.pth\"\ndata_path = '/kaggle/input/finale-dataset/final_dataset/localization'","metadata":{"id":"htDpcMFDYmp7","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:18:17.121549Z","iopub.execute_input":"2025-11-19T20:18:17.121780Z","iopub.status.idle":"2025-11-19T20:18:17.134378Z","shell.execute_reply.started":"2025-11-19T20:18:17.121757Z","shell.execute_reply":"2025-11-19T20:18:17.133512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Check if a GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize the dataset\ndataset = LocalizationDataset(data_path, target_size=(img_width, img_height), augment=False)\n\n# Define dataset splitting\ntrain_size = int(0.7 * len(dataset))\nval_size = int(0.1 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\n\n\n# Create train, validation, and test datasets\ntrain_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n\n# Create DataLoaders for batching\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=8,\n    pin_memory=True,\n    persistent_workers=True,\n    prefetch_factor=4\n)\n\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n\n# Initialize the simple model\nmodel = CCLN().to(device)\n\n# Create a random input tensor\nrandom_input = torch.randn(3, 3, img_height, img_width).to(device)\n\n# Pass the random input through the model\noutput = model(random_input)\n\n# Print the input and output shapes\nprint(\"Input shape:\", random_input.shape)\nprint(\"Output shape:\", output.shape)\n\n# Define the optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#, weight_decay=1e-4) \nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.9)","metadata":{"id":"uaXKZTRkYo8o","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:18:17.135602Z","iopub.execute_input":"2025-11-19T20:18:17.135905Z","iopub.status.idle":"2025-11-19T20:18:22.361497Z","shell.execute_reply.started":"2025-11-19T20:18:17.135879Z","shell.execute_reply":"2025-11-19T20:18:22.360578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def denormalize_boxes(boxes, image_width, image_height):\n    \"\"\"De-normalize bounding boxes from [0, 1] to pixel values.\"\"\"\n    boxes[:, 0] *= image_width  # x_min\n    boxes[:, 1] *= image_height  # y_min\n    boxes[:, 2] *= image_width  # x_max\n    boxes[:, 3] *= image_height  # y_max\n    return boxes\n\ndef normalize_boxes(boxes, image_width, image_height):\n    \"\"\"Normalize bounding boxes from [0, 1] to pixel values.\"\"\"\n    boxes[:, 0] /= image_width  # x_min\n    boxes[:, 1] /= image_height  # y_min\n    boxes[:, 2] /= image_width  # x_max\n    boxes[:, 3] /= image_height  # y_max\n    return boxes\n\n# List of loaders and their names for display\nloaders = [train_loader, val_loader, test_loader]\nloader_names = ['Train', 'Validation', 'Test']\n\n# Example usage in your validation loop\nfor loader, name in zip(loaders, loader_names):\n    idx = random.randint(0, len(loader.dataset) - 1)\n    sample = loader.dataset[idx]\n    image = sample['image'].unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        pred_boxes = model(image)\n\n    # Get image dimensions\n    image_width, image_height = image.shape[3], image.shape[2]\n\n    # De-normalize predicted boxes if they are normalized\n    pred_boxes = denormalize_boxes(pred_boxes.cpu(), image_width, image_height)\n    true_boxes = denormalize_boxes(sample['bbox'].unsqueeze(0).cpu(), image_width, image_height)\n\n    # Calculate IoU loss\n    loss = 1 - torchvision.ops.box_iou(pred_boxes, true_boxes)\n\n    # Normalize predicted boxes if they are normalized\n    pred_boxes = normalize_boxes(pred_boxes.cpu(), image_width, image_height)\n    true_boxes = normalize_boxes(sample['bbox'].unsqueeze(0).cpu(), image_width, image_height)\n\n    # Display the results\n    display_image_with_boxes(sample['image'], true_boxes, pred_boxes, loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:18:22.362474Z","iopub.execute_input":"2025-11-19T20:18:22.362713Z","iopub.status.idle":"2025-11-19T20:18:23.407409Z","shell.execute_reply.started":"2025-11-19T20:18:22.362688Z","shell.execute_reply":"2025-11-19T20:18:23.406483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport time\nimport os\nimport torch\nsnapshot_path = \"/kaggle/input/ccln/pytorch/default/1/ccln.pth\"\nif os.path.exists(snapshot_path):\n    model, _ = load_checkpoint(model, snapshot_path, device)\n    model.eval()\n\ndef calculate_model_complexity(model):\n    \"\"\"Calculates the number of parameters and the disk size of the model.\"\"\"\n    \n    # 1. Number of parameters (trainable)\n    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    # 2. Model size (MB) - saves model state to disk temporarily\n    temp_path = 'temp_model_size.pth'\n    torch.save(model.state_dict(), temp_path)\n    model_size_bytes = os.path.getsize(temp_path)\n    model_size_mb = model_size_bytes / (1024 * 1024)\n    os.remove(temp_path)\n    \n    print(\"\\n--- Model Complexity Metrics ---\")\n    print(f\"Number of parameters: {total_params:,}\")\n    print(f\"Model size (MB): {model_size_mb:.2f} MB\")\n    \n    return total_params, model_size_mb\n\n\ndef calculate_inference_speed(model, data_loader, device, num_warmup_batches=5):\n    \"\"\"Calculates FPS and Latency, ensuring accurate timing for CPU/GPU inference.\"\"\"\n    model.eval()\n    \n    # 1. Warm-up Runs (Crucial for accurate GPU timing)\n    print(\"Starting warm-up...\")\n    with torch.no_grad():\n        for i, batch in enumerate(data_loader):\n            if i >= num_warmup_batches:\n                break\n            images = batch['image'].to(device)\n            labels = batch['bbox'].to(device)\n            outputs = model(images)\n            for i in range(images.size(0)):\n                true_box = labels[i].unsqueeze(0)  # Shape [1, 4]\n                pred_box = outputs[i].unsqueeze(0)  # Shape [1, 4]\n\n                # Optionally display predictions and ground truth\n                loss = 1 - torchvision.ops.box_iou(pred_box, true_box)\n                display_image_with_boxes(images[i].cpu(), true_box.cpu(), pred_box.cpu(), loss)\n            if device.type == 'cuda':\n                torch.cuda.synchronize()\n    \n    # 2. Main Timing Loop\n    total_time = 0.0\n    total_images = 0\n    \n    if device.type == 'cuda':\n        starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n    \n    print(\"Starting main timing...\")\n    with torch.no_grad():\n        for batch in data_loader:\n            images = batch['image'].to(device)\n            batch_size = images.size(0)\n            \n            # --- Start Timing (only inference time) ---\n            if device.type == 'cuda':\n                starter.record()\n                _ = model(images)\n                ender.record()\n                torch.cuda.synchronize()\n                curr_time = starter.elapsed_time(ender) / 1000.0 # Convert ms to seconds\n            else:\n                start_time = time.time()\n                _ = model(images)\n                curr_time = time.time() - start_time\n            # --- End Timing ---\n            \n            total_time += curr_time\n            total_images += batch_size\n\n    # 3. Calculate metrics\n    fps = total_images / total_time\n    latency_ms = (total_time / total_images) * 1000\n    \n    print(\"\\n--- Inference Speed Metrics ---\")\n    print(f\"Total images processed: {total_images}\")\n    print(f\"Total inference time: {total_time:.4f} seconds\")\n    print(f\"FPS (Frames Per Second): {fps:.2f}\")\n    print(f\"Latency (ms per image): {latency_ms:.2f} ms\")\n    \n    return fps, latency_ms\n\n# --- Usage Example ---\n\n# # 1. Calculate and display complexity metrics\nnum_params, model_size = calculate_model_complexity(model)\n\n# # 2. Calculate and display inference speed metrics\nfps, latency = calculate_inference_speed(model, test_loader, device)\n\n\n# # 3. Then run your evaluation logic (like your original evaluate_and_plot function)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:18:23.408679Z","iopub.execute_input":"2025-11-19T20:18:23.408979Z","iopub.status.idle":"2025-11-19T20:18:37.962874Z","shell.execute_reply.started":"2025-11-19T20:18:23.408952Z","shell.execute_reply":"2025-11-19T20:18:37.961782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Assuming the function returns train_losses, val_losses, train_accuracies, val_accuracies\n#best_model_state, best_epoch, train_losses, val_losses, train_accuracies, val_accuracies = train_model(model, train_loader, val_loader, optimizer, device, num_epochs, model_save_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:18:37.964251Z","iopub.execute_input":"2025-11-19T20:18:37.964548Z","iopub.status.idle":"2025-11-19T20:18:37.968907Z","shell.execute_reply.started":"2025-11-19T20:18:37.964518Z","shell.execute_reply":"2025-11-19T20:18:37.968081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses = []\nvalidation_losses = []\ntrain_accuracies = []\nvalidation_accuracies = []\n\nmodel_path = model_save_path\nbest_loss = float('inf')\n\n\ndef train(model, train_loader, validation_loader, optimizer, scheduler,\n          num_epochs=16, display_every=10):\n\n    best_model_state = None\n    best_val_loss = float('inf')\n    best_val_iou = 0.0\n\n    for epoch in range(num_epochs):\n\n        # -----------------------\n        # TRAIN\n        # -----------------------\n        model.train()\n        total_loss = 0\n        total_iou = 0\n\n        lr = optimizer.param_groups[0]['lr']\n\n        for batch in train_loader:\n            images = batch['image'].to(device)\n            bboxes = batch['bbox'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n\n            loss = torchvision.ops.complete_box_iou_loss(outputs, bboxes, reduction='mean')\n\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.detach()\n            iou = torchvision.ops.box_iou(outputs, bboxes).diag().mean()\n            total_iou += iou.detach()\n\n        avg_train_loss = total_loss / len(train_loader)\n        avg_train_iou = total_iou / len(train_loader)\n\n        train_losses.append(avg_train_loss.item())\n        train_accuracies.append(avg_train_iou.item())\n\n        # -----------------------\n        # VALIDATION\n        # -----------------------\n        model.eval()\n        val_loss = 0\n        val_iou = 0\n\n        with torch.no_grad():\n            for batch in validation_loader:\n                images = batch['image'].to(device)\n                bboxes = batch['bbox'].to(device)\n\n                outputs = model(images)\n                loss = torchvision.ops.complete_box_iou_loss(outputs, bboxes, reduction='mean')\n\n                val_loss += loss.detach()\n                val_iou += torchvision.ops.box_iou(outputs, bboxes).diag().mean().detach()\n\n        avg_val_loss = val_loss / len(validation_loader)\n        avg_val_iou = val_iou / len(validation_loader)\n\n        validation_losses.append(avg_val_loss.item())\n        validation_accuracies.append(avg_val_iou.item())\n\n        scheduler.step(avg_val_loss)\n\n        # PRINT METRICS\n        print(f\"Epoch [{epoch+1}/{num_epochs}] | LR {lr:.2e} \"\n              f\"| Train Loss {avg_train_loss:.4f} | Val Loss {avg_val_loss:.4f} \"\n              f\"| Train IoU {avg_train_iou:.4f} | Val IoU {avg_val_iou:.4f}\")\n\n        # -----------------------\n        # SAVE BEST MODEL\n        # -----------------------\n        is_best = (avg_val_loss < best_val_loss) or (avg_val_iou > best_val_iou)\n\n        if is_best:\n            print(\"  âœ” New best model! Saving...\")\n            best_model_state = model.state_dict()\n            best_val_loss = avg_val_loss\n            best_val_iou = avg_val_iou\n\n            torch.save(\n                {\n                    \"epoch\": epoch + 1,\n                    \"model\": model.state_dict(),\n                    \"optimizer\": optimizer.state_dict(),\n                    \"loss\": avg_val_loss,\n                    \"iou\": avg_val_iou,\n                },\n                model_path  # global save path\n            )\n\n        # -----------------------\n        # SAVE SNAPSHOT EVERY 10 EPOCHS\n        # -----------------------\n        #if (epoch + 1) % 10 == 0:\n        #    snapshot_path = \"/kaggle/input/ccln/pytorch/default/1/ccln.pth\"\n        #    torch.save(model.state_dict(), snapshot_path)\n        #    print(f\"  ðŸ“Œ Snapshot saved: {snapshot_path}\")\n\n        # -----------------------\n        # DISPLAY 5 RANDOM VAL IMAGES\n        # -----------------------\n        if (epoch + 1) % display_every == 0:\n            print(\"  ðŸ–¼ Displaying 5 random validation predictions...\")\n            self_display_count = 0\n\n            for batch in validation_loader:\n                images = batch['image']\n                bboxes = batch['bbox']\n                outputs = model(images.to(device)).cpu()\n\n                for i in range(images.size(0)):\n                    if self_display_count >= 5:\n                        break\n                    loss = 1 - torchvision.ops.box_iou(\n                        outputs[i].unsqueeze(0), \n                        bboxes[i].unsqueeze(0)\n                    )\n\n                    display_image_with_boxes(\n                        images[i], \n                        bboxes[i].unsqueeze(0), \n                        outputs[i].unsqueeze(0), \n                        loss\n                    )\n                    self_display_count += 1\n\n                if self_display_count >= 5:\n                    break\n\n    return best_model_state\n\n\n# --- Execution ---\n# Note: You must ensure 'model_save_path' and 'num_epochs' are defined before this call.\n# The scheduler is now a required argument for the train function.\n\n# Initialize the scheduler before calling train\n# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.9) # assumed to be initialized outside\n\n# Call train function with the new argument\nbest_model_state = train(model, train_loader, val_loader, optimizer, scheduler, num_epochs=num_epochs)\n\nepochs = range(1, len(train_losses) + 1)\n\n# Plot Loss vs Epoch\nplt.figure(figsize=(12, 6))\nplt.plot(epochs, train_losses, label='Training Loss', color='blue')\nplt.plot(epochs, validation_losses, label='Validation Loss', color='orange')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.grid()\nplt.show()\n\n# Plot Accuracy vs Epoch\nplt.figure(figsize=(12, 6))\nplt.plot(epochs, [t_a  for t_a in train_accuracies], label='Training Accuracy', color='green')\nplt.plot(epochs, [v_a for v_a in validation_accuracies], label='Validation Accuracy', color='red')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.grid()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:18:37.970270Z","iopub.execute_input":"2025-11-19T20:18:37.970532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the model\nsnapshot_path = \"/kaggle/input/ccln/pytorch/default/1/ccln.pth\"\nloaded_model, _ = load_checkpoint(model, snapshot_path, device)\n#loaded_model, _ = load_checkpoint(model, \"/kaggle/input/ccln_0.75.pth/pytorch/default/1/clnn_0.75.pth\", device)\nloaded_model.eval()\nmodel = loaded_model\n\n# Evaluate the model on the test set\ntest_loss, test_iou = evaluate_model(loaded_model, test_loader, device)\nprint(f\"Test Loss: {test_loss:.4f}, Test IoU: {test_iou:.4f}\")\n\n","metadata":{"id":"w4kEBRdkYga7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\n# Randomly select 5 unique indices from the test dataset\nnum_samples = 5\nidxs = random.sample(range(len(test_loader.dataset)), num_samples)\n\nfor idx in idxs:\n    # Get the sample from the test dataset\n    sample = test_loader.dataset[idx]  # Assuming test_loader.dataset returns the sample directly\n    image = sample['image'].unsqueeze(0).to(device)  # Add batch dimension and move to device\n\n    # Make predictions\n    with torch.no_grad():  # Disable gradient calculation for inference\n        pred_boxes = model(image)  # Get predicted bounding boxes\n\n    # Convert predictions to CPU and numpy for visualization\n    pred_boxes = pred_boxes.cpu()\n\n    # Calculate IoU loss\n    loss = 1 - torchvision.ops.box_iou(pred_boxes, sample['bbox'].unsqueeze(0))\n    \n    # Display the image with the ground truth and predicted bounding boxes\n    display_image_with_boxes(sample['image'], sample['bbox'].unsqueeze(0), pred_boxes, loss)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision.ops import box_iou\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport torchvision.transforms.functional as TF\n\n# Metric calculation using torch's box_iou\ndef calculate_metrics(pred_boxes, true_boxes, iou_threshold=0.75):\n    # Ensure bounding boxes are valid\n    pred_boxes = validate_boxes(pred_boxes)\n    true_boxes = validate_boxes(true_boxes)\n\n    # Compute IoU using torchvision's box_iou\n    ious = torchvision.ops.box_iou(pred_boxes, true_boxes)\n\n    # True Positives (TP): IoU > threshold\n    tp = (ious > iou_threshold).sum().item()\n\n    # False Positives (FP): Predicted boxes not matching any true box\n    fp = pred_boxes.size(0) - tp\n\n    # False Negatives (FN): True boxes not matched by any predicted box\n    fn = true_boxes.size(0) - tp\n\n    # Total predictions for accuracy\n    total = pred_boxes.size(0) + true_boxes.size(0)\n\n    # Calculate precision, recall, F1 score, and accuracy\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    accuracy = tp / total if total > 0 else 0\n\n    return precision, recall, f1, accuracy\n\n# Average Precision calculation with dynamic IoU\ndef calculate_average_precision(pred_boxes, true_boxes, iou_thresholds=np.arange(0.0, 1.1, 0.1)):\n    # Ensure bounding boxes are valid\n    pred_boxes = validate_boxes(pred_boxes)\n    true_boxes = validate_boxes(true_boxes)\n\n    # Compute IoU\n    ious = torchvision.ops.box_iou(pred_boxes, true_boxes).flatten().cpu().numpy()\n\n    # Prepare for precision-recall calculation\n    avg_precisions = []\n    for iou_thresh in iou_thresholds:\n        labels = (ious > iou_thresh).astype(int)\n        precision, recall, _ = precision_recall_curve(labels, ious)\n        avg_precision = average_precision_score(labels, ious)\n        avg_precisions.append(avg_precision)\n\n    return avg_precisions\n\n# Plot Precision-Recall curve\ndef plot_precision_recall_curve(precision, recall, average_precision):\n    plt.figure()\n    plt.plot(recall, precision, label=f'AP = {average_precision:.2f}')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    plt.legend()\n    plt.show()\n\n# Plot metrics (Precision, Recall, F1-score, Accuracy, etc.) vs IoU thresholds\ndef plot_metrics_vs_iou_thresholds(pred_boxes, true_boxes, thresholds=np.linspace(0.5, 0.95, 10)):\n    precisions, recalls, f1_scores, accuracies, avg_precisions = [], [], [], [], []\n\n    for iou_thresh in thresholds:\n        precision, recall, f1, accuracy = calculate_metrics(pred_boxes, true_boxes, iou_thresh)\n        avg_precision = calculate_average_precision(pred_boxes, true_boxes)[-1]  # Last value for max IoU\n\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1)\n        accuracies.append(accuracy)\n        avg_precisions.append(avg_precision)\n\n    # Plot results\n    plt.figure(figsize=(12, 8))\n    plt.plot(thresholds, precisions, label='Precision')\n    plt.plot(thresholds, recalls, label='Recall')\n    plt.plot(thresholds, f1_scores, label='F1-Score')\n    plt.plot(thresholds, accuracies, label='Accuracy')\n    plt.plot(thresholds, avg_precisions, label='Average Precision')\n    plt.xlabel('IoU Threshold')\n    plt.ylabel('Score')\n    plt.title('Metrics vs IoU Threshold')\n    plt.legend()\n    plt.show()\n\n# Validate bounding boxes to ensure coordinates are within [0, 1]\ndef validate_boxes(boxes):\n    boxes[:, :2] = torch.clamp(boxes[:, :2], 0, 1)  # Clamp x_min, y_min\n    boxes[:, 2:] = torch.clamp(boxes[:, 2:], 0, 1)  # Clamp x_max, y_max\n    return boxes\n\n# Evaluate and plot metrics\ndef evaluate_and_plot(test_loader, model, device):\n    all_precisions, all_recalls, all_f1s, all_accuracies = [], [], [], []\n    all_true_boxes, all_pred_boxes = [], []\n\n    with torch.no_grad():\n        for batch in test_loader:\n            images = batch['image'].to(device)\n            true_boxes = batch['bbox'].to(device)\n            predicted_boxes = model(images)\n\n            for i in range(images.size(0)):\n                true_box = true_boxes[i].unsqueeze(0)  # Shape [1, 4]\n                pred_box = predicted_boxes[i].unsqueeze(0)  # Shape [1, 4]\n\n                precision, recall, f1, accuracy = calculate_metrics(pred_box, true_box, iou_threshold=0.75)\n                all_precisions.append(precision)\n                all_recalls.append(recall)\n                all_f1s.append(f1)\n                all_accuracies.append(accuracy)\n\n                all_true_boxes.append(true_box)\n                all_pred_boxes.append(pred_box)\n\n                # Optionally display predictions and ground truth\n                display_image_with_boxes(images[i].cpu(), true_box.cpu(), pred_box.cpu())\n\n    # Calculate metrics vs IoU thresholds\n    plot_metrics_vs_iou_thresholds(torch.cat(all_pred_boxes), torch.cat(all_true_boxes))\n\n    # Precision-Recall curve with dynamic IoU\n    avg_precisions = calculate_average_precision(torch.cat(all_pred_boxes), torch.cat(all_true_boxes))\n    for iou in np.arange(0.0, 1.1, 0.1):\n        precision, recall, _ = precision_recall_curve((box_iou(torch.cat(all_pred_boxes), torch.cat(all_true_boxes)).flatten() > iou).int().cpu().numpy(), \n                                                       box_iou(torch.cat(all_pred_boxes), torch.cat(all_true_boxes)).flatten().cpu().numpy())\n        plot_precision_recall_curve(precision, recall, avg_precisions[int(iou * 10)])\n\n    # Summary of average metrics\n    print(\"Average Metrics at IoU 0.75:\")\n    print(f\"Precision: {np.mean(all_precisions):.2f}\")\n    print(f\"Recall: {np.mean(all_recalls):.2f}\")\n    print(f\"F1-Score: {np.mean(all_f1s):.2f}\")\n    print(f\"Accuracy: {np.mean(all_accuracies):.2f}\")\n\n# Undo normalization for image display\ndef undo_normalization(image, mean, std):\n    for t, m, s in zip(image, mean, std):\n        t.mul_(s).add_(m)\n    return image\n\n# Visualization helper\ndef display_image_with_boxes(image, true_box, pred_box):\n    print(\"Prediction:\", pred_box)\n    print(\"Label:\", true_box)\n\n    # Undo normalization\n    mean = torch.tensor([0.485, 0.456, 0.406])\n    std = torch.tensor([0.229, 0.224, 0.225])\n    #image = undo_normalization(image, mean, std)\n\n    # Convert to numpy for plotting\n    image = image.permute(1, 2, 0).numpy()  # Convert to HWC\n\n    # Create a figure and axis\n    plt.figure(figsize=(12, 12))\n    plt.imshow(image)\n\n    # Define colors for bounding boxes\n    colors = ['red', 'green']  # Red for true box, Green for predicted box\n\n    # Draw bounding boxes for true and predicted boxes\n    for i, bbox in enumerate([true_box.squeeze(0), pred_box.squeeze(0)]):\n        x_min, y_min, x_max, y_max = bbox\n        x_min = int(x_min * image.shape[1])  # Scale to image width\n        y_min = int(y_min * image.shape[0])  # Scale to image height\n        x_max = int(x_max * image.shape[1])  # Scale to image width\n        y_max = int(y_max * image.shape[0])  # Scale to image height\n\n        # Draw rectangle\n        plt.gca().add_patch(plt.Rectangle(\n            (x_min, y_min),\n            x_max - x_min,\n            y_max - y_min,\n            edgecolor=colors[i],\n            facecolor='none',\n            linewidth=2,\n            label='True Box' if i == 0 else 'Predicted Box'\n        ))\n\n    # Add legend and hide axes\n    plt.legend()\n    plt.axis('off')\n    plt.show()\n\n# Example usage\nevaluate_and_plot(test_loader, model, device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}