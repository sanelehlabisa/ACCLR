{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "9685b9be-cbdb-401a-8744-861d51d028c1",
    "_uuid": "c032a5c3-8586-4255-b627-0b3c154fffac",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-22T19:55:21.861911Z",
     "iopub.status.busy": "2025-11-22T19:55:21.861053Z",
     "iopub.status.idle": "2025-11-22T19:55:21.868408Z",
     "shell.execute_reply": "2025-11-22T19:55:21.867534Z",
     "shell.execute_reply.started": "2025-11-22T19:55:21.861870Z"
    },
    "id": "XzE4KR6wBCEm",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# PyTorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# torchvision Imports\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision import models\n",
    "from torchvision.ops import complete_box_iou_loss\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f46adb83-576c-44fe-9570-b4f240bdb653",
    "_uuid": "dbf3c944-04dc-4cc8-a7cf-0c94e9019cb1",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "889b1220-9489-47bd-b750-d17b30b0c12e",
    "_uuid": "1a0f80d1-5d75-4ca4-a633-0490524cfb2c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-22T19:55:21.870993Z",
     "iopub.status.busy": "2025-11-22T19:55:21.870142Z",
     "iopub.status.idle": "2025-11-22T19:55:21.890827Z",
     "shell.execute_reply": "2025-11-22T19:55:21.890042Z",
     "shell.execute_reply.started": "2025-11-22T19:55:21.870960Z"
    },
    "id": "WetMpvSuB5z8",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LocalizationDataset(Dataset):\n",
    "    def __init__(self, dataset_folder, target_size=(256, 256)):\n",
    "        self.target_size = target_size\n",
    "\n",
    "        # Will store PIL images or image paths\n",
    "        self.images = []      # list of PIL images (augmented) or strings (original paths)\n",
    "        self.bboxes = []      # list of 4-dim tensors (normalized)\n",
    "\n",
    "        image_folder = os.path.join(dataset_folder, \"images\")\n",
    "        label_folder = os.path.join(dataset_folder, \"labels\")\n",
    "\n",
    "        # Base transform\n",
    "        self.base_transform = transforms.Compose([\n",
    "            transforms.Resize(target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        for img_name in os.listdir(image_folder):\n",
    "            if not img_name.endswith(\".jpg\"):\n",
    "                continue\n",
    "\n",
    "            img_path = os.path.join(image_folder, img_name)\n",
    "            xml_path = os.path.join(label_folder, img_name.replace(\".jpg\", \".xml\"))\n",
    "            if not os.path.exists(xml_path):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                bbox, w, h = self.parse_xml(xml_path)\n",
    "\n",
    "                nb = [\n",
    "                    bbox[0] / w, bbox[1] / h,\n",
    "                    bbox[2] / w, bbox[3] / h\n",
    "                ]\n",
    "\n",
    "                # STORE ORIGINAL IMAGE PATH\n",
    "                self.images.append(img_path)\n",
    "                self.bboxes.append(torch.tensor(nb, dtype=torch.float))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    def parse_xml(self, xml_file):\n",
    "            \"\"\"Parse the XML file to extract bounding box coordinates and image dimensions.\"\"\"\n",
    "            tree = ET.parse(xml_file)\n",
    "            root = tree.getroot()\n",
    "    \n",
    "            # Extract image dimensions\n",
    "            width = int(root.find('size/width').text)\n",
    "            height = int(root.find('size/height').text)\n",
    "    \n",
    "            # Extract bounding box information (assumes a single object per image)\n",
    "            for obj in root.findall('object'):\n",
    "                bbox = obj.find('bndbox')\n",
    "                xmin = int(bbox.find('xmin').text)\n",
    "                ymin = int(bbox.find('ymin').text)\n",
    "                xmax = int(bbox.find('xmax').text)\n",
    "                ymax = int(bbox.find('ymax').text)\n",
    "    \n",
    "                return [xmin, ymin, xmax, ymax], width, height\n",
    "    \n",
    "            raise ValueError(f\"No bounding box found in {xml_file}\")\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # __getitem__\n",
    "    # ---------------------------------------------------------\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.images[idx]\n",
    "\n",
    "        # item is either a path (string) OR a PIL image (augmentation)\n",
    "        if isinstance(item, str):\n",
    "            img = Image.open(item).convert(\"RGB\")\n",
    "        else:\n",
    "            img = item\n",
    "\n",
    "        img = self.base_transform(img)\n",
    "        bbox = self.bboxes[idx]\n",
    "\n",
    "        return {\"image\": img, \"bbox\": bbox}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "c3b1dd12-ab86-4db7-ab36-0ed8c73ea507",
    "_uuid": "3bf8f475-2042-4e87-b951-fb4c283a468f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-22T19:55:21.892073Z",
     "iopub.status.busy": "2025-11-22T19:55:21.891828Z",
     "iopub.status.idle": "2025-11-22T19:55:21.907924Z",
     "shell.execute_reply": "2025-11-22T19:55:21.907073Z",
     "shell.execute_reply.started": "2025-11-22T19:55:21.892049Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import os \n",
    "\n",
    "# IMPORTANT: Use the correct path for your Kaggle input\n",
    "KAGGLE_RESNET_PATH = \"/kaggle/input/resnet_50/pytorch/default/1/resnet50-11ad3fa6.pth\"\n",
    "\n",
    "class CCLN(nn.Module):\n",
    "    def __init__(self, pretrained_weights_path=KAGGLE_RESNET_PATH):\n",
    "        super(CCLN, self).__init__()\n",
    "\n",
    "        # ... (Backbone Loading remains the same) ...\n",
    "        resnet50 = models.resnet50(weights=None) \n",
    "        if pretrained_weights_path and os.path.exists(pretrained_weights_path):\n",
    "             state_dict = torch.load(pretrained_weights_path, map_location='cpu', weights_only=True)\n",
    "             resnet50.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "        # --- Model Components (Backbone) ---\n",
    "        self.conv1 = resnet50.conv1; self.bn1 = resnet50.bn1; self.relu = resnet50.relu; self.maxpool = resnet50.maxpool\n",
    "        self.layer1 = resnet50.layer1; self.layer2 = resnet50.layer2; self.layer3 = resnet50.layer3; self.layer4 = resnet50.layer4     \n",
    "        \n",
    "        # --- Activations ---\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "        # ... (Decoder Layers remain the same) ...\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv_concat1 = nn.Conv2d(2048 + 1024, 512, kernel_size=1); self.bn_concat1 = nn.BatchNorm2d(512)\n",
    "        self.conv_up1 = nn.Conv2d(512, 512, kernel_size=3, padding=1); self.bn_up1 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv_concat2 = nn.Conv2d(512 + 512, 256, kernel_size=1); self.bn_concat2 = nn.BatchNorm2d(256)\n",
    "        self.conv_up2 = nn.Conv2d(256, 256, kernel_size=3, padding=1); self.bn_up2 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.upsample3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv_concat3 = nn.Conv2d(256 + 256, 128, kernel_size=1); self.bn_concat3 = nn.BatchNorm2d(128)\n",
    "        self.conv_up3 = nn.Conv2d(128, 128, kernel_size=3, padding=1); self.bn_up3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # --- FINAL REGRESSION HEAD (CONV-BASED, SMALLER, AND STABLE) ---\n",
    "        # 1. Final 1x1 Conv to predict 4 channels (x_min, y_min, x_max, y_max) per pixel\n",
    "        self.conv_final = nn.Conv2d(128, 4, kernel_size=1)\n",
    "        nn.init.constant_(self.conv_final.bias, 0.0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ... (Backbone and Decoder layers are unchanged) ...\n",
    "        x = self.conv1(x); x = self.bn1(x); x = self.relu(x); r0 = self.maxpool(x)\n",
    "        r1 = self.layer1(r0); r2 = self.layer2(r1); r3 = self.layer3(r2); r4 = self.layer4(r3)\n",
    "\n",
    "        # Decoder Path (Leaky ReLU)\n",
    "        x = self.upsample1(r4); x = torch.cat([x, r3], dim=1); x = self.conv_concat1(x); x = self.leaky_relu(self.bn_concat1(x))\n",
    "        x = self.conv_up1(x); x = self.leaky_relu(self.bn_up1(x))\n",
    "\n",
    "        x = self.upsample2(x); x = torch.cat([x, r2], dim=1); x = self.conv_concat2(x); x = self.leaky_relu(self.bn_concat2(x))\n",
    "        x = self.conv_up2(x); x = self.leaky_relu(self.bn_up2(x))\n",
    "\n",
    "        x = self.upsample3(x); x = torch.cat([x, r1], dim=1); x = self.conv_concat3(x); x = self.leaky_relu(self.bn_concat3(x))\n",
    "        x = self.conv_up3(x); x = self.leaky_relu(self.bn_up3(x)) # x is (B, 128, H/4, W/4)\n",
    "\n",
    "        # --- FINAL REGRESSION HEAD (CONV-BASED) ---\n",
    "        # 1. Final Conv (B, 4, H/4, W/4)\n",
    "        x = self.conv_final(x)\n",
    "        \n",
    "        # 2. Sigmoid on the predicted values\n",
    "        x = torch.sigmoid(x) \n",
    "\n",
    "        # 3. Global average pooling to get the final (xmin, ymin, xmax, ymax)\n",
    "        output = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "37612bb9-4d5b-4c2d-99bc-4fc344bc2def",
    "_uuid": "829359cc-614c-4ce2-80cc-586d5c80bf8e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-22T19:55:21.952731Z",
     "iopub.status.busy": "2025-11-22T19:55:21.952468Z",
     "iopub.status.idle": "2025-11-22T19:55:21.960738Z",
     "shell.execute_reply": "2025-11-22T19:55:21.959899Z",
     "shell.execute_reply.started": "2025-11-22T19:55:21.952706Z"
    },
    "id": "6HcD8ZeuqqlZ",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CCLN3(nn.Module):\n",
    "    def __init__(self, weights=True):\n",
    "        super(CCLN3, self).__init__()\n",
    "\n",
    "        # 1. Load the model architecture\n",
    "        resnet = resnet50(weights=None)  # don't trigger download\n",
    "        \n",
    "        # 2. Load state_dict from your uploaded file\n",
    "        state_dict_path = '/kaggle/input/resnet_50/pytorch/default/1/resnet50-11ad3fa6.pth'\n",
    "        state_dict = torch.load(state_dict_path, map_location='cpu', weights_only=True)\n",
    "        \n",
    "        # 3. Load weights into the model\n",
    "        resnet.load_state_dict(state_dict)\n",
    "\n",
    "        # Use all layers except the final fully connected layer\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # Extract up to the last conv block\n",
    "\n",
    "        # Upsampling and concatenation layers\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv_up1 = nn.Conv2d(2048, 1024, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv_up2 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upsample3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv_up3 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        # Final output layer (4 bounding box coordinates)\n",
    "        self.conv_out = nn.Conv2d(256, 4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature extraction using ResNet-50 backbone\n",
    "        x = self.backbone(x)  # Output: [batch_size, 2048, 10, 10] for 320x320 input\n",
    "\n",
    "        # Upsampling layers\n",
    "        x = self.upsample1(x)\n",
    "        x = F.relu(self.conv_up1(x))\n",
    "\n",
    "        x = self.upsample2(x)\n",
    "        x = F.relu(self.conv_up2(x))\n",
    "\n",
    "        x = self.upsample3(x)\n",
    "        x = F.relu(self.conv_up3(x))\n",
    "\n",
    "        # Output layer (bounding box coordinates)\n",
    "        x = torch.sigmoid(self.conv_out(x))  # Normalize to [0, 1]\n",
    "\n",
    "        # Flatten the output to [batch_size, 4]\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))  # Global average pooling to (1, 1)\n",
    "        x = x.view(x.size(0), -1)  # Flatten to [batch_size, 4]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b4c59032-114c-44c0-b985-9b716dbd4706",
    "_uuid": "65ccd27e-db21-42ae-bf2b-1352706a3afd",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "08c01d31-a2d7-4777-a341-4457775711fe",
    "_uuid": "db9f1d14-6be6-42f9-b2f0-f1ffc68bf2ff",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-22T19:55:21.962883Z",
     "iopub.status.busy": "2025-11-22T19:55:21.962564Z",
     "iopub.status.idle": "2025-11-22T19:55:21.975762Z",
     "shell.execute_reply": "2025-11-22T19:55:21.974943Z",
     "shell.execute_reply.started": "2025-11-22T19:55:21.962858Z"
    },
    "id": "Dys269vrNXTs",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def display_image_with_boxes(image, true_box, pred_box, iou_loss):\n",
    "    print(\"Prediction:\", pred_box)\n",
    "    print(\"Label:\", true_box)\n",
    "    print(\"IoU:\", torchvision.ops.box_iou(true_box, pred_box).item())\n",
    "\n",
    "    # Convert the PyTorch tensor to NumPy and reshape (H, W, C)\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()  # Convert to NumPy array and change shape to (H, W, C)\n",
    "\n",
    "    # Create a figure and axis\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(image)\n",
    "\n",
    "    # Define colors for bounding boxes\n",
    "    colors = ['red', 'green']  # Red for true box, Green for predicted box\n",
    "\n",
    "    # Draw bounding boxes for true and predicted boxes\n",
    "    for i, bbox in enumerate([true_box.squeeze(0), pred_box.squeeze(0)]):  # Squeeze to remove extra dimensions\n",
    "        # Ensure bbox is a tensor and convert to float for safety\n",
    "        bbox = bbox.cpu().detach().numpy()  # Move to CPU, detach from computation graph, and convert to NumPy\n",
    "        bbox = [float(coord) for coord in bbox]  # Convert to float\n",
    "\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "\n",
    "        # Scale to image dimensions\n",
    "        x_min = int(x_min * image.shape[1])  # Scale to image width\n",
    "        y_min = int(y_min * image.shape[0])  # Scale to image height\n",
    "        x_max = int(x_max * image.shape[1])  # Scale to image width\n",
    "        y_max = int(y_max * image.shape[0])  # Scale to image height\n",
    "\n",
    "        # Draw the rectangle using Matplotlib\n",
    "        plt.gca().add_patch(plt.Rectangle(\n",
    "            (x_min, y_min),\n",
    "            x_max - x_min,\n",
    "            y_max - y_min,\n",
    "            edgecolor=colors[i],\n",
    "            facecolor='none',\n",
    "            linewidth=2,\n",
    "            label='True Box' if i == 0 else 'Predicted Box'\n",
    "        ))\n",
    "\n",
    "    # Add IoU loss text at the bottom right corner\n",
    "    plt.text(\n",
    "        image.shape[1] - 200, image.shape[0] - 50,\n",
    "        f\"IoU Loss: {iou_loss.item():.4f}\",\n",
    "        color='white',\n",
    "        fontsize=14,\n",
    "        bbox=dict(facecolor='black', alpha=0.8, edgecolor='none', pad=6)\n",
    "    )\n",
    "\n",
    "    # Add legend and hide axes\n",
    "    plt.legend()\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "826ad8ec-f9a7-459b-bb27-b5b751511dd0",
    "_uuid": "9f36169d-ae55-4667-a93b-86c7b19d861c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "d6cb66b7-24d6-4c94-9f98-c82be11111c0",
    "_uuid": "e283a045-9011-4517-a207-9b066c2a556c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-22T19:55:21.977179Z",
     "iopub.status.busy": "2025-11-22T19:55:21.976945Z",
     "iopub.status.idle": "2025-11-22T19:55:21.993262Z",
     "shell.execute_reply": "2025-11-22T19:55:21.992533Z",
     "shell.execute_reply.started": "2025-11-22T19:55:21.977156Z"
    },
    "id": "obLMSSVzQXM0",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(pred_boxes, true_boxes, iou_threshold=0.5):\n",
    "    ious = torchvision.ops.box_iou(pred_boxes, true_boxes)\n",
    "\n",
    "    # True Positives, False Positives, False Negatives\n",
    "    tp = (ious > iou_threshold).sum().item()\n",
    "    fp = (ious <= iou_threshold).sum().item()\n",
    "    fn = (true_boxes.shape[0] - tp)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calculate_average_precision(pred_boxes, true_boxes):\n",
    "    # Flatten for average precision computation\n",
    "    ious = torchvision.ops.box_iou(pred_boxes, true_boxes).cpu().numpy().flatten()\n",
    "\n",
    "    # Binary classification for IoU > threshold (positive), otherwise (negative)\n",
    "    labels = (ious > 0.5).astype(int)\n",
    "\n",
    "    # Use precision_recall_curve and average_precision_score from sklearn\n",
    "    precision, recall, _ = precision_recall_curve(labels, ious)\n",
    "    average_precision = average_precision_score(labels, ious)\n",
    "\n",
    "    return precision, recall, average_precision\n",
    "\n",
    "def plot_precision_recall_curve(precision, recall, average_precision):\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label=f'Average Precision = {average_precision:.2f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def compute_accuracy(predictions, targets, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the predictions based on IoU.\n",
    "\n",
    "    Args:\n",
    "        predictions: Predicted bounding boxes (tensor of shape [N, 4]).\n",
    "        targets: Ground truth bounding boxes (tensor of shape [N, 4]).\n",
    "        threshold: IoU threshold to consider a prediction as correct.\n",
    "\n",
    "    Returns:\n",
    "        accuracy: The proportion of correct predictions based on IoU.\n",
    "    \"\"\"\n",
    "    # Ensure predictions and targets are in the correct shape\n",
    "    if predictions.ndim == 1:\n",
    "        predictions = predictions.unsqueeze(0)\n",
    "    if targets.ndim == 1:\n",
    "        targets = targets.unsqueeze(0)\n",
    "\n",
    "    # Calculate IoU for each predicted box with the corresponding target box\n",
    "    ious = (predictions, targets)\n",
    "\n",
    "    # Count the number of correct predictions based on the threshold\n",
    "    correct_predictions = (ious > threshold).sum().item()\n",
    "    total_predictions = predictions.size(0)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "def plot_metrics_vs_iou_thresholds(pred_boxes, true_boxes, thresholds=np.linspace(0.5, 0.95, 10)):\n",
    "    precisions, recalls, f1_scores, accuracies, average_precisions = [], [], [], [], []\n",
    "\n",
    "    for iou_thresh in thresholds:\n",
    "        precision, recall, f1 = calculate_metrics(pred_boxes, true_boxes, iou_thresh)\n",
    "        avg_precision = calculate_average_precision(pred_boxes, true_boxes)[2]\n",
    "\n",
    "        accuracies.append((precision + recall) / 2)  # Simple accuracy\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        average_precisions.append(avg_precision)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(thresholds, precisions, label='Precision')\n",
    "    plt.plot(thresholds, recalls, label='Recall')\n",
    "    plt.plot(thresholds, f1_scores, label='F1-Score')\n",
    "    plt.plot(thresholds, accuracies, label='Accuracy')\n",
    "    plt.plot(thresholds, average_precisions, label='Average Precision')\n",
    "\n",
    "    plt.xlabel('IoU Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Metrics vs IoU Threshold')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "15b98bd9-0f5d-479f-bb03-e8e5502789a6",
    "_uuid": "0436d4f1-a06f-4a41-a288-88926b8ea6fd",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "f66c0b13-155b-4a5d-8e23-08a515e11435",
    "_uuid": "984800a3-6814-4815-bd49-64561a13c298",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-22T19:55:21.994686Z",
     "iopub.status.busy": "2025-11-22T19:55:21.994416Z",
     "iopub.status.idle": "2025-11-22T19:55:22.017813Z",
     "shell.execute_reply": "2025-11-22T19:55:22.016944Z",
     "shell.execute_reply.started": "2025-11-22T19:55:21.994655Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, device, epochs, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Trains a localization model, computes IoU-based accuracy, and saves the best model.\n",
    "    Prints training loss, validation loss, and accuracy on one line per epoch.\n",
    "\n",
    "    Args:\n",
    "        model: The model to train.\n",
    "        train_loader: DataLoader for training data.\n",
    "        val_loader: DataLoader for validation data.\n",
    "        optimizer: Optimizer for training.\n",
    "        device: Device to train on (e.g., 'cuda' or 'cpu').\n",
    "        epochs: Number of epochs to train.\n",
    "        checkpoint_path: Path to save the best model checkpoint.\n",
    "    \"\"\"\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    model.train()\n",
    "    for c_path in [checkpoint_path, \"/kaggle/input/ccln_0.75.pth/pytorch/default/1/clnn.pth\", \"/kaggle/input/ccln_0.75.pth/pytorch/default/1/clnn_0.75.pth\"]:\n",
    "        if False and os.path.exists(c_path):\n",
    "            model, best_val_loss = load_checkpoint(model, c_path, device)\n",
    "            optimizer.load_state_dict(torch.load(c_path, weights_only=True)['optimizer_state_dict'])\n",
    "            start_epoch = torch.load(c_path, weights_only=True)['epoch'] + 1\n",
    "            break\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for batch in train_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            bboxes = batch['bbox'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate Complete Box IoU loss\n",
    "            #loss = torchvision.ops.complete_box_iou_loss(outputs, bboxes, reduction='mean') + torch.nn.functional.mse_loss(outputs, bboxes).mean()\n",
    "            loss = torchvision.ops.complete_box_iou_loss(outputs, bboxes, reduction='mean')\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Average training loss for the epoch\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation loss and accuracy\n",
    "        val_loss, val_accuracy = evaluate_model(model, val_loader, device)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Store IoU accuracy for training\n",
    "        train_accuracies.append(torchvision.ops.box_iou(outputs, bboxes).mean().item())\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        #current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}] | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Accuracy (IoU): {val_accuracy:.4f}')\n",
    "\n",
    "        # Save the model if the validation loss is the best so far\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            save_checkpoint(model, optimizer, checkpoint_path, best_val_loss, epoch)\n",
    "            if val_accuracy > 0.75:\n",
    "                save_checkpoint(model, optimizer, f\"/kaggle/working/clnn_{val_accuracy:.2f}.pth\", best_val_loss, epoch)\n",
    "\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    print(f'Training completed. Best model saved with validation loss: {best_val_loss:.4f}')\n",
    "    return best_model_state, epoch, train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a validation dataset and computes IoU-based accuracy and loss.\n",
    "\n",
    "    Args:\n",
    "        model: The model to evaluate.\n",
    "        data_loader: DataLoader for validation data.\n",
    "        device: Device to evaluate on.\n",
    "\n",
    "    Returns:\n",
    "        val_loss (float): The mean loss over the validation dataset.\n",
    "        accuracy (float): The mean IoU accuracy over the validation dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    iou_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            bboxes = batch['bbox'].to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate Complete Box IoU loss\n",
    "            loss = torchvision.ops.complete_box_iou_loss(outputs, bboxes, reduction='mean')\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Compute IoU for each pair of predicted and ground truth bounding boxes\n",
    "            iou_scores.append(torchvision.ops.box_iou(outputs, bboxes).mean().item())\n",
    "\n",
    "    # Compute mean IoU accuracy and loss over all batches\n",
    "    val_loss = running_loss / len(data_loader)\n",
    "    val_accuracy = sum(iou_scores) / len(iou_scores)\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "def save_checkpoint(model, optimizer, checkpoint_path, best_loss, epoch):\n",
    "    \"\"\"\n",
    "    Saves the model checkpoint.\n",
    "\n",
    "    Args:\n",
    "        model: The model to save.\n",
    "        optimizer: The optimizer state to save.\n",
    "        checkpoint_path: Path to save the model checkpoint.\n",
    "        best_loss: Best validation loss to include in the saved checkpoint.\n",
    "        epoch: Current epoch number.\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_loss': best_loss,\n",
    "        'epoch': epoch\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Model saved with Best Validation Loss: {best_loss:.4f}\")\n",
    "\n",
    "def load_checkpoint(model, checkpoint_path, device):\n",
    "    \"\"\"\n",
    "    Loads the model checkpoint.\n",
    "\n",
    "    Args:\n",
    "        model: The model to load the checkpoint into.\n",
    "        checkpoint_path: Path to the saved model checkpoint.\n",
    "        device: Device to load the model on.\n",
    "\n",
    "    Returns:\n",
    "        model: The loaded model (unchanged if checkpoint not found).\n",
    "        best_loss: The best validation loss from the checkpoint, or infinity if not found.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(checkpoint_path):\n",
    "        print(f\"Checkpoint file not found: {checkpoint_path}. Returning original model and best_loss=inf.\")\n",
    "        return model, float('inf')\n",
    "\n",
    "    print(f\"Loading model from: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    if 'model_state_dict' in checkpoint.keys():\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        print(\"Warning: 'model_state_dict' key not found in checkpoint. Model weights not loaded.\")\n",
    "\n",
    "    if 'best_loss' in checkpoint.keys():\n",
    "        best_loss = checkpoint['best_loss']\n",
    "    else:\n",
    "        print(\"Warning: 'best_loss' key not found in checkpoint. Setting best_loss = infinity.\")\n",
    "        best_loss = float('inf')\n",
    "\n",
    "    return model, best_loss\n",
    "    \n",
    "def plot_metrics(train_accuracies, val_accuracies, train_losses, val_losses, checkpoint_path, epoch):\n",
    "    \"\"\"\n",
    "    Plots the training and validation accuracy and loss curves.\n",
    "\n",
    "    Args:\n",
    "        train_accuracies: List of training accuracies.\n",
    "        val_accuracies: List of validation accuracies.\n",
    "        train_losses: List of training losses.\n",
    "        val_losses: List of validation losses.\n",
    "        checkpoint_path: Path to save the plot.\n",
    "        epoch: Current epoch.\n",
    "    \"\"\"\n",
    "    # Plotting training and validation accuracies\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, epoch + 1), train_accuracies, label='Train Accuracy', color='blue')\n",
    "    plt.plot(range(1, epoch + 1), val_accuracies, label='Validation Accuracy', color='orange')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(checkpoint_path, f'accuracy_plot_epoch_{epoch}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Plotting training and validation losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, epoch + 1), train_losses, label='Train Loss', color='blue')\n",
    "    plt.plot(range(1, epoch + 1), val_losses, label='Validation Loss', color='orange')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(checkpoint_path, f'loss_plot_epoch_{epoch}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_top_bottom_images(model, test_loader, device, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Plots the top 5 and bottom 5 images based on IoU loss.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model.\n",
    "        test_loader: DataLoader for the test data.\n",
    "        device: Device to use for inference.\n",
    "        checkpoint_path: Path to save the plots.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    iou_losses = []\n",
    "    images = []\n",
    "    bboxes = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch_images = batch['image'].to(device)\n",
    "            batch_bboxes = batch['bbox'].to(device)\n",
    "\n",
    "            outputs = model(batch_images)\n",
    "            loss = 1 - torchvision.ops.box_iou(outputs, batch_bboxes).mean()\n",
    "            iou_losses.extend(loss.cpu().numpy())\n",
    "            images.extend(batch_images.cpu())\n",
    "            bboxes.extend(batch_bboxes.cpu())\n",
    "\n",
    "    # Sort the images based on IoU loss\n",
    "    sorted_indices = np.argsort(iou_losses)\n",
    "\n",
    "    # Plot the top 5 and bottom 5 images\n",
    "    fig, axs = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "    # Top 5 images\n",
    "    for i in range(5):\n",
    "        index = sorted_indices[i]\n",
    "        ax = axs[0, i]\n",
    "        display_image_with_boxes(images[index], bboxes[index], outputs[index], iou_losses[index])\n",
    "        ax.set_title(f\"Top {i+1}, IoU Loss: {iou_losses[index]:.4f}\")\n",
    "\n",
    "    # Bottom 5 images\n",
    "    for i in range(5):\n",
    "        index = sorted_indices[-i-1]\n",
    "        ax = axs[1, i]\n",
    "        display_image_with_boxes(images[index], bboxes[index], outputs[index], iou_losses[index])\n",
    "        ax.set_title(f\"Bottom {i+1}, IoU Loss: {iou_losses[index]:.4f}\")\n",
    "\n",
    "    plt.savefig(os.path.join(checkpoint_path, 'top_bottom_images.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "d189c735-c98a-4767-8b54-03d6dd478723",
    "_uuid": "e99cae03-4bf2-4f1e-8ce7-c3c4eee14483",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-22T19:55:22.019793Z",
     "iopub.status.busy": "2025-11-22T19:55:22.019509Z",
     "iopub.status.idle": "2025-11-22T19:55:22.032364Z",
     "shell.execute_reply": "2025-11-22T19:55:22.031594Z",
     "shell.execute_reply.started": "2025-11-22T19:55:22.019768Z"
    },
    "id": "htDpcMFDYmp7",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters and settings\n",
    "img_channel = 3  # RGB\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "learning_rate = 0.001\n",
    "batch_size = 4\n",
    "num_epochs = 128\n",
    "model_save_path = \"/kaggle/working/ccln.pth\"\n",
    "save_input_model_path = \"/kaggle/input/ccln/pytorch/default/1/clnn.pth\"\n",
    "data_path = '/kaggle/input/acclr-dataset/acclr_dataset/localization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a07a07ab-e507-40f7-978d-d76f0105889d",
    "_uuid": "0988f9a9-a573-4531-b884-b13508d74acf",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b21b8ed3-666e-4974-9238-47be0804a76b",
    "_uuid": "39b1ddf0-4154-44f9-a529-e01d65b6a636",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-22T19:55:22.033575Z",
     "iopub.status.busy": "2025-11-22T19:55:22.033340Z",
     "iopub.status.idle": "2025-11-22T19:55:30.678440Z",
     "shell.execute_reply": "2025-11-22T19:55:30.677500Z",
     "shell.execute_reply.started": "2025-11-22T19:55:22.033552Z"
    },
    "id": "uaXKZTRkYo8o",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = LocalizationDataset(data_path, target_size=(img_width, img_height))\n",
    "\n",
    "# Define dataset splitting\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "\n",
    "# Create train, validation, and test datasets\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Initialize the simple model\n",
    "model = CCLN().to(device)\n",
    "\n",
    "if os.path.exists(save_input_model_path):\n",
    "    model, _ = load_checkpoint(model, save_input_model_path, device)\n",
    "\n",
    "# Create a random input tensor\n",
    "random_input = torch.randn(3, 3, img_height, img_width).to(device)\n",
    "\n",
    "# Pass the random input through the model\n",
    "output = model(random_input)\n",
    "\n",
    "# Print the input and output shapes\n",
    "print(\"Input shape:\", random_input.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "# Define the optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#, weight_decay=1e-2) \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "ed9ab279-86cc-4a4c-93da-c267f3e1789b",
    "_uuid": "5c288941-8fa8-42cf-a99a-04ce864c91cb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-22T19:55:30.679987Z",
     "iopub.status.busy": "2025-11-22T19:55:30.679610Z",
     "iopub.status.idle": "2025-11-22T19:55:31.585549Z",
     "shell.execute_reply": "2025-11-22T19:55:31.584601Z",
     "shell.execute_reply.started": "2025-11-22T19:55:30.679956Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def denormalize_boxes(boxes, image_width, image_height):\n",
    "    \"\"\"De-normalize bounding boxes from [0, 1] to pixel values.\"\"\"\n",
    "    boxes[:, 0] *= image_width  # x_min\n",
    "    boxes[:, 1] *= image_height  # y_min\n",
    "    boxes[:, 2] *= image_width  # x_max\n",
    "    boxes[:, 3] *= image_height  # y_max\n",
    "    return boxes\n",
    "\n",
    "def normalize_boxes(boxes, image_width, image_height):\n",
    "    \"\"\"Normalize bounding boxes from [0, 1] to pixel values.\"\"\"\n",
    "    boxes[:, 0] /= image_width  # x_min\n",
    "    boxes[:, 1] /= image_height  # y_min\n",
    "    boxes[:, 2] /= image_width  # x_max\n",
    "    boxes[:, 3] /= image_height  # y_max\n",
    "    return boxes\n",
    "\n",
    "# List of loaders and their names for display\n",
    "loaders = [train_loader, val_loader, test_loader]\n",
    "loader_names = ['Train', 'Validation', 'Test']\n",
    "\n",
    "# Example usage in your validation loop\n",
    "for loader, name in zip(loaders, loader_names):\n",
    "    idx = random.randint(0, len(loader.dataset) - 1)\n",
    "    sample = loader.dataset[idx]\n",
    "    image = sample['image'].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_boxes = model(image)\n",
    "\n",
    "    # Get image dimensions\n",
    "    image_width, image_height = image.shape[3], image.shape[2]\n",
    "\n",
    "    # De-normalize predicted boxes if they are normalized\n",
    "    pred_boxes = denormalize_boxes(pred_boxes.cpu(), image_width, image_height)\n",
    "    true_boxes = denormalize_boxes(sample['bbox'].unsqueeze(0).cpu(), image_width, image_height)\n",
    "\n",
    "    # Calculate IoU loss\n",
    "    loss = 1 - torchvision.ops.box_iou(pred_boxes, true_boxes)\n",
    "\n",
    "    # Normalize predicted boxes if they are normalized\n",
    "    pred_boxes = normalize_boxes(pred_boxes.cpu(), image_width, image_height)\n",
    "    true_boxes = normalize_boxes(sample['bbox'].unsqueeze(0).cpu(), image_width, image_height)\n",
    "\n",
    "    # Display the results\n",
    "    display_image_with_boxes(sample['image'], true_boxes, pred_boxes, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "9e08cb87-e3b3-4e5a-ab41-be51346e310c",
    "_uuid": "2d53fd9b-781d-4b6e-8291-97a0f1aaa969",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-22T19:55:31.586943Z",
     "iopub.status.busy": "2025-11-22T19:55:31.586681Z",
     "iopub.status.idle": "2025-11-22T19:55:31.592643Z",
     "shell.execute_reply": "2025-11-22T19:55:31.591793Z",
     "shell.execute_reply.started": "2025-11-22T19:55:31.586916Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "9dee13d4-ad59-43ad-9d56-e66e166966fb",
    "_uuid": "66231a4c-927c-4860-b1b5-43d7ad1095ed",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-22T19:55:31.595041Z",
     "iopub.status.busy": "2025-11-22T19:55:31.594742Z",
     "iopub.status.idle": "2025-11-22T19:55:47.673002Z",
     "shell.execute_reply": "2025-11-22T19:55:47.671766Z",
     "shell.execute_reply.started": "2025-11-22T19:55:31.595015Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "snapshot_path = \"/kaggle/input/ccln/pytorch/default/1/ccln.pth\"\n",
    "if os.path.exists(snapshot_path):\n",
    "    model, _ = load_checkpoint(model, snapshot_path, device)\n",
    "    model.eval()\n",
    "\n",
    "def calculate_model_complexity(model):\n",
    "    \"\"\"Calculates the number of parameters and the disk size of the model.\"\"\"\n",
    "    \n",
    "    # 1. Number of parameters (trainable)\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # 2. Model size (MB) - saves model state to disk temporarily\n",
    "    temp_path = 'temp_model_size.pth'\n",
    "    torch.save(model.state_dict(), temp_path)\n",
    "    model_size_bytes = os.path.getsize(temp_path)\n",
    "    model_size_mb = model_size_bytes / (1024 * 1024)\n",
    "    os.remove(temp_path)\n",
    "    \n",
    "    print(\"\\n--- Model Complexity Metrics ---\")\n",
    "    print(f\"Number of parameters: {total_params:,}\")\n",
    "    print(f\"Model size (MB): {model_size_mb:.2f} MB\")\n",
    "    \n",
    "    return total_params, model_size_mb\n",
    "\n",
    "\n",
    "def calculate_inference_speed(model, data_loader, device, num_warmup_batches=5):\n",
    "    \"\"\"Calculates FPS and Latency, ensuring accurate timing for CPU/GPU inference.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Warm-up Runs (Crucial for accurate GPU timing)\n",
    "    print(\"Starting warm-up...\")\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            if i >= num_warmup_batches:\n",
    "                break\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['bbox'].to(device)\n",
    "            outputs = model(images)\n",
    "            for i in range(images.size(0)):\n",
    "                true_box = labels[i].unsqueeze(0)  # Shape [1, 4]\n",
    "                pred_box = outputs[i].unsqueeze(0)  # Shape [1, 4]\n",
    "\n",
    "                # Optionally display predictions and ground truth\n",
    "                loss = 1 - torchvision.ops.box_iou(pred_box, true_box)\n",
    "                display_image_with_boxes(images[i].cpu(), true_box.cpu(), pred_box.cpu(), loss)\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "    \n",
    "    # 2. Main Timing Loop\n",
    "    total_time = 0.0\n",
    "    total_images = 0\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    print(\"Starting main timing...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            # --- Start Timing (only inference time) ---\n",
    "            if device.type == 'cuda':\n",
    "                starter.record()\n",
    "                _ = model(images)\n",
    "                ender.record()\n",
    "                torch.cuda.synchronize()\n",
    "                curr_time = starter.elapsed_time(ender) / 1000.0 # Convert ms to seconds\n",
    "            else:\n",
    "                start_time = time.time()\n",
    "                _ = model(images)\n",
    "                curr_time = time.time() - start_time\n",
    "            # --- End Timing ---\n",
    "            \n",
    "            total_time += curr_time\n",
    "            total_images += batch_size\n",
    "\n",
    "    # 3. Calculate metrics\n",
    "    fps = total_images / total_time\n",
    "    latency_ms = (total_time / total_images) * 1000\n",
    "    \n",
    "    print(\"\\n--- Inference Speed Metrics ---\")\n",
    "    print(f\"Total images processed: {total_images}\")\n",
    "    print(f\"Total inference time: {total_time:.4f} seconds\")\n",
    "    print(f\"FPS (Frames Per Second): {fps:.2f}\")\n",
    "    print(f\"Latency (ms per image): {latency_ms:.2f} ms\")\n",
    "    \n",
    "    return fps, latency_ms\n",
    "\n",
    "# --- Usage Example ---\n",
    "\n",
    "# # 1. Calculate and display complexity metrics\n",
    "num_params, model_size = calculate_model_complexity(model)\n",
    "\n",
    "# # 2. Calculate and display inference speed metrics\n",
    "fps, latency = calculate_inference_speed(model, test_loader, device)\n",
    "\n",
    "\n",
    "# # 3. Then run your evaluation logic (like your original evaluate_and_plot function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f84a14f4-2cc4-45c8-a237-e3e028b67f71",
    "_uuid": "20ace7df-238d-4543-b113-7f18b59e4449",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "f07dd80b-1852-4744-8930-6eac1e88bfec",
    "_uuid": "ba06c28f-df8a-46f4-9a0e-6ab40b5dfdbd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-22T19:55:47.675417Z",
     "iopub.status.busy": "2025-11-22T19:55:47.674986Z",
     "iopub.status.idle": "2025-11-22T19:55:47.679910Z",
     "shell.execute_reply": "2025-11-22T19:55:47.679018Z",
     "shell.execute_reply.started": "2025-11-22T19:55:47.675370Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Assuming the function returns train_losses, val_losses, train_accuracies, val_accuracies\n",
    "#best_model_state, best_epoch, train_losses, val_losses, train_accuracies, val_accuracies = train_model(model, train_loader, val_loader, optimizer, device, num_epochs, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f24aacd3-1b88-47f5-bd6c-83f406df0084",
    "_uuid": "3e27de52-a032-4119-bfc5-02692e806fa7",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a3d305f5-0239-43e3-ba62-61a1761dece3",
    "_uuid": "bc6aedd5-bba8-4c9b-b435-bb81271a32d8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-22T19:55:47.682298Z",
     "iopub.status.busy": "2025-11-22T19:55:47.681465Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "model_path = model_save_path\n",
    "best_loss = float('inf')\n",
    "\n",
    "\n",
    "def train(model, train_loader, validation_loader, optimizer, scheduler,\n",
    "          num_epochs=16, display_every=10):\n",
    "\n",
    "    best_model_state = None\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_iou = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # -----------------------\n",
    "        # TRAIN\n",
    "        # -----------------------\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_iou = 0\n",
    "\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        for batch in train_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            bboxes = batch['bbox'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = torchvision.ops.complete_box_iou_loss(outputs, bboxes, reduction='mean')\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.detach()\n",
    "            iou = torchvision.ops.box_iou(outputs, bboxes).diag().mean()\n",
    "            total_iou += iou.detach()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        avg_train_iou = total_iou / len(train_loader)\n",
    "\n",
    "        train_losses.append(avg_train_loss.item())\n",
    "        train_accuracies.append(avg_train_iou.item())\n",
    "\n",
    "        # -----------------------\n",
    "        # VALIDATION\n",
    "        # -----------------------\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_iou = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_loader:\n",
    "                images = batch['image'].to(device)\n",
    "                bboxes = batch['bbox'].to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = torchvision.ops.complete_box_iou_loss(outputs, bboxes, reduction='mean')\n",
    "\n",
    "                val_loss += loss.detach()\n",
    "                val_iou += torchvision.ops.box_iou(outputs, bboxes).diag().mean().detach()\n",
    "\n",
    "        avg_val_loss = val_loss / len(validation_loader)\n",
    "        avg_val_iou = val_iou / len(validation_loader)\n",
    "\n",
    "        validation_losses.append(avg_val_loss.item())\n",
    "        validation_accuracies.append(avg_val_iou.item())\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # PRINT METRICS\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | LR {lr:.2e} \"\n",
    "              f\"| Train Loss {avg_train_loss:.4f} | Val Loss {avg_val_loss:.4f} \"\n",
    "              f\"| Train IoU {avg_train_iou:.4f} | Val IoU {avg_val_iou:.4f}\")\n",
    "\n",
    "        # -----------------------\n",
    "        # SAVE BEST MODEL\n",
    "        # -----------------------\n",
    "        is_best = (avg_val_loss < best_val_loss) or (avg_val_iou > best_val_iou)\n",
    "\n",
    "        if is_best:\n",
    "            print(\"   New best model! Saving...\")\n",
    "            best_model_state = model.state_dict()\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_val_iou = avg_val_iou\n",
    "\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"loss\": avg_val_loss,\n",
    "                    \"iou\": avg_val_iou,\n",
    "                },\n",
    "                model_path  # global save path\n",
    "            )\n",
    "\n",
    "        # -----------------------\n",
    "        # SAVE SNAPSHOT EVERY 10 EPOCHS\n",
    "        # -----------------------\n",
    "        #if (epoch + 1) % 10 == 0:\n",
    "        #    snapshot_path = \"/kaggle/input/ccln/pytorch/default/1/ccln.pth\"\n",
    "        #    torch.save(model.state_dict(), snapshot_path)\n",
    "        #    print(f\"   Snapshot saved: {snapshot_path}\")\n",
    "\n",
    "        # -----------------------\n",
    "        # DISPLAY 5 RANDOM VAL IMAGES\n",
    "        # -----------------------\n",
    "        if (epoch + 1) % display_every == 0:\n",
    "            print(\"   Displaying 5 random validation predictions...\")\n",
    "            self_display_count = 0\n",
    "\n",
    "            for batch in validation_loader:\n",
    "                images = batch['image']\n",
    "                bboxes = batch['bbox']\n",
    "                outputs = model(images.to(device)).cpu()\n",
    "\n",
    "                for i in range(images.size(0)):\n",
    "                    if self_display_count >= 5:\n",
    "                        break\n",
    "                    loss = 1 - torchvision.ops.box_iou(\n",
    "                        outputs[i].unsqueeze(0), \n",
    "                        bboxes[i].unsqueeze(0)\n",
    "                    )\n",
    "\n",
    "                    display_image_with_boxes(\n",
    "                        images[i], \n",
    "                        bboxes[i].unsqueeze(0), \n",
    "                        outputs[i].unsqueeze(0), \n",
    "                        loss\n",
    "                    )\n",
    "                    self_display_count += 1\n",
    "\n",
    "                if self_display_count >= 5:\n",
    "                    break\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "# Note: You must ensure 'model_save_path' and 'num_epochs' are defined before this call.\n",
    "# The scheduler is now a required argument for the train function.\n",
    "\n",
    "# Initialize the scheduler before calling train\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.9) # assumed to be initialized outside\n",
    "\n",
    "# Call train function with the new argument\n",
    "best_model_state = train(model, train_loader, val_loader, optimizer, scheduler, num_epochs=num_epochs)\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "# Plot Loss vs Epoch\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(epochs, validation_losses, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy vs Epoch\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, [t_a  for t_a in train_accuracies], label='Training Accuracy', color='green')\n",
    "plt.plot(epochs, [v_a for v_a in validation_accuracies], label='Validation Accuracy', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "86d1c1b1-a26e-4bf2-98c5-553ee0dd04f0",
    "_uuid": "4146742c-0995-49cb-8651-5162bf4fe8ef",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "49c08b0a-bf30-439d-9412-fc801ef2df5a",
    "_uuid": "2f284b67-8b0b-4563-8413-c222df82d99b",
    "collapsed": false,
    "id": "w4kEBRdkYga7",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "snapshot_path = \"/kaggle/input/ccln/pytorch/default/1/ccln.pth\"\n",
    "loaded_model, _ = load_checkpoint(model, snapshot_path, device)\n",
    "#loaded_model, _ = load_checkpoint(model, \"/kaggle/input/ccln_0.75.pth/pytorch/default/1/clnn_0.75.pth\", device)\n",
    "loaded_model.eval()\n",
    "model = loaded_model\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_iou = evaluate_model(loaded_model, test_loader, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test IoU: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6d2c0040-658b-4dc6-a0f7-cbabbd2e9fee",
    "_uuid": "92a79bfd-22c0-46f8-b69a-a6963834dfdf",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Randomly select 5 unique indices from the test dataset\n",
    "num_samples = 5\n",
    "idxs = random.sample(range(len(test_loader.dataset)), num_samples)\n",
    "\n",
    "for idx in idxs:\n",
    "    # Get the sample from the test dataset\n",
    "    sample = test_loader.dataset[idx]  # Assuming test_loader.dataset returns the sample directly\n",
    "    image = sample['image'].unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        pred_boxes = model(image)  # Get predicted bounding boxes\n",
    "\n",
    "    # Convert predictions to CPU and numpy for visualization\n",
    "    pred_boxes = pred_boxes.cpu()\n",
    "\n",
    "    # Calculate IoU loss\n",
    "    loss = 1 - torchvision.ops.box_iou(pred_boxes, sample['bbox'].unsqueeze(0))\n",
    "    \n",
    "    # Display the image with the ground truth and predicted bounding boxes\n",
    "    display_image_with_boxes(sample['image'], sample['bbox'].unsqueeze(0), pred_boxes, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e81d6c96-785a-4af9-b4f3-56a940f86600",
    "_uuid": "6247dd9e-0a20-4e6f-815b-75d961478905",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-22T22:23:43.891654Z",
     "iopub.status.busy": "2025-11-22T22:23:43.890817Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPLETE CCLN EVALUATION CELL\n",
    "# ============================================\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.ops import box_iou\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import time, os\n",
    "from tqdm import tqdm\n",
    "#from ptflops import get_model_complexity_info\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Helper: Convert predicted normalized box to pixel box\n",
    "# ----------------------------------------------------------\n",
    "def to_pixel(bbox, W, H):\n",
    "    \"\"\"bbox = [x1,y1,x2,y2] normalized; convert to pixel coords.\"\"\"\n",
    "    bx = bbox.detach().cpu().numpy()\n",
    "    if np.max(bx) <= 1.0:\n",
    "        x1 = int(bx[0] * W)\n",
    "        y1 = int(bx[1] * H)\n",
    "        x2 = int(bx[2] * W)\n",
    "        y2 = int(bx[3] * H)\n",
    "    else:\n",
    "        x1,y1,x2,y2 = bx.astype(int)\n",
    "    return [x1,y1,x2,y2]\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Helper: Draw GT + Pred boxes\n",
    "# ----------------------------------------------------------\n",
    "def draw_boxes(img, gt, pred, iou):\n",
    "    img_np = img.permute(1,2,0).cpu().numpy()\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(img_np)\n",
    "    \n",
    "    for box,color,label in [(gt,'red',f\"GT\"),(pred,'lime',f\"Pred\")]:\n",
    "        x1,y1,x2,y2 = box\n",
    "        w,h = x2-x1, y2-y1\n",
    "        rect = plt.Rectangle((x1,y1), w,h, fill=False, edgecolor=color, linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "    plt.title(f\"IoU:{iou:.3f}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Compute Per-image Metrics\n",
    "# ----------------------------------------------------------\n",
    "def compute_metrics(iou, thr):\n",
    "    tp = 1 if iou >= thr else 0\n",
    "    fp = 1 - tp\n",
    "    fn = 1 - tp\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp+fp)>0 else 0\n",
    "    recall    = tp / (tp + fn) if (tp+fn)>0 else 0\n",
    "    f1        = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0\n",
    "    accuracy  = tp\n",
    "\n",
    "    return precision, recall, f1, accuracy\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# MAIN EVALUATION\n",
    "# ----------------------------------------------------------\n",
    "def evaluate_ccln(model, test_loader, device, visualize=False):\n",
    "\n",
    "    print(\"\\n========== MODEL INFO ==========\")\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Parameters: {params:,}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"tmp.pth\")\n",
    "    size_mb = os.path.getsize(\"tmp.pth\")/(1024*1024)\n",
    "    os.remove(\"tmp.pth\")\n",
    "    print(f\"Model size: {size_mb:.2f} MB\")\n",
    "\n",
    "    #macs, params_fl = get_model_complexity_info(\n",
    "    #    model, (3,256,256), verbose=False, print_per_layer_stat=False\n",
    "    #)\n",
    "    #print(f\"MACs: {macs:,}   FLOPs: {2*macs:,}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Collect IoUs\n",
    "    # ------------------------------\n",
    "    ious = []\n",
    "\n",
    "    print(\"\\n========== RUNNING INFERENCE ==========\")\n",
    "\n",
    "    # GPU warmup\n",
    "    if device.type==\"cuda\":\n",
    "        dummy = torch.randn(1,3,256,256).to(device)\n",
    "        for _ in range(5):\n",
    "            _ = model(dummy)\n",
    "\n",
    "    total_time = 0\n",
    "    total_imgs = 0\n",
    "\n",
    "    for batch in tqdm(test_loader):\n",
    "        imgs  = batch[\"image\"].to(device)\n",
    "        gt_bb = batch[\"bbox\"]     # normalized\n",
    "\n",
    "        for i in range(imgs.size(0)):\n",
    "            img = imgs[i]\n",
    "            gt  = gt_bb[i]\n",
    "\n",
    "            # --- Model forward ---\n",
    "            t0 = time.time()\n",
    "            pred = model(img.unsqueeze(0))\n",
    "            if device.type==\"cuda\": torch.cuda.synchronize()\n",
    "            t1 = time.time()\n",
    "\n",
    "            total_time += (t1-t0)\n",
    "            total_imgs += 1\n",
    "\n",
    "            # convert both boxes to pixel coords\n",
    "            _,H,W = img.shape\n",
    "            gt_px   = to_pixel(gt, W, H)\n",
    "            pred_px = to_pixel(pred.squeeze(0), W, H)\n",
    "\n",
    "            # compute IoU\n",
    "            iou = box_iou(\n",
    "                torch.tensor([pred_px],dtype=torch.float32),\n",
    "                torch.tensor([gt_px],dtype=torch.float32)\n",
    "            ).item()\n",
    "            ious.append(iou)\n",
    "\n",
    "            if visualize:\n",
    "                draw_boxes(img, gt_px, pred_px, iou)\n",
    "\n",
    "    ious = np.array(ious)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Compute metrics for IoU thresholds\n",
    "    # ------------------------------\n",
    "    thresholds = np.arange(0.1,1.0,0.1)\n",
    "    precisions, recalls, f1s, accs = [],[],[],[]\n",
    "\n",
    "    print(\"\\n========== METRICS (By IoU Threshold) ==========\")\n",
    "\n",
    "    for thr in thresholds:\n",
    "        p,r,f,a = compute_metrics(ious>=thr, thr)\n",
    "        precisions.append(np.mean(p))\n",
    "        recalls.append(np.mean(r))\n",
    "        f1s.append(np.mean(f))\n",
    "        accs.append(np.mean(a))\n",
    "\n",
    "    # ------------------------------\n",
    "    # PR CURVE (standard)\n",
    "    # ------------------------------\n",
    "    print(\"\\nDrawing Precision-Recall Curve (using IoU as confidence)...\")\n",
    "\n",
    "    # treat IoU as confidence score\n",
    "    y_true = np.ones(len(ious))          # always 1 GT per img\n",
    "    y_scores = ious                      # confidence = IoU\n",
    "\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_scores)\n",
    "    ap = average_precision_score(y_true, y_scores)\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(recall_curve, precision_curve, label=f\"AP={ap:.3f}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve (IoU as Confidence)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ------------------------------\n",
    "    # Plot metrics vs IoU threshold\n",
    "    # ------------------------------\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(thresholds, precisions, label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls, label=\"Recall\")\n",
    "    plt.plot(thresholds, f1s, label=\"F1\")\n",
    "    plt.plot(thresholds, accs, label=\"Accuracy\")\n",
    "    plt.xlabel(\"IoU Threshold\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Metrics vs IoU Threshold\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # ------------------------------\n",
    "    # FPS\n",
    "    # ------------------------------\n",
    "    fps = total_imgs / total_time\n",
    "    print(f\"\\n========== FINAL RESULTS ==========\")\n",
    "    print(f\"Mean IoU: {ious.mean():.3f}\")\n",
    "    print(f\"FPS: {fps:.2f}\")\n",
    "    print(f\"AP: {ap:.3f}\")\n",
    "    print(f\"Precision@0.75: {precisions[6]:.3f}\")\n",
    "    print(f\"Recall@0.75: {recalls[6]:.3f}\")\n",
    "    print(f\"F1@0.75: {f1s[6]:.3f}\")\n",
    "    print(f\"Accuracy@0.75: {accs[6]:.3f}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------\n",
    "# >>> CALL IT LIKE THIS:\n",
    "evaluate_ccln(model, test_loader, device, visualize=True)\n",
    "# -----------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "955be6ce-e475-411f-b458-67609a45ca52",
    "_uuid": "6e4758d3-26ca-40ed-ab28-c6cca9996a9b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-22T22:18:47.429151Z",
     "iopub.status.busy": "2025-11-22T22:18:47.428524Z",
     "iopub.status.idle": "2025-11-22T22:22:05.502717Z",
     "shell.execute_reply": "2025-11-22T22:22:05.501698Z",
     "shell.execute_reply.started": "2025-11-22T22:18:47.429116Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install ptflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "571d9454-ceed-4f38-8169-dac04c065e1f",
    "_uuid": "16dfdbb1-4f67-4f0d-839a-fbd6c0beb0da",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ebd0b093-0660-4c1e-a7dd-2d770c060b50",
    "_uuid": "f00a6c46-bad5-4272-8d68-d3d2554b2a4c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8808651,
     "sourceId": 13831985,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 172454,
     "modelInstanceId": 149959,
     "sourceId": 176128,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 460729,
     "modelInstanceId": 444236,
     "sourceId": 593603,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 460801,
     "modelInstanceId": 444306,
     "sourceId": 593680,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 509316,
     "modelInstanceId": 493922,
     "sourceId": 653753,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
